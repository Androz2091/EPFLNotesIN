\documentclass{article}

% Expanded on 2021-10-04 at 13:09:57.

\usepackage{../../style}

\title{Algèbre linéaire 1}
\author{Joachim Favre}
\date{Lundi 04 octobre 2021}

\begin{document}
\maketitle

\lecture{4}{2021-10-04}{Dépendance linéaire}{
\begin{itemize}[left=0pt]
    \item Démonstration que les solutions d'un système peuvent être écrites sous la forme d'une somme d'une solution particulière et de la solution générale au système homogène correspondant.
    \item Définition du concept de dépendance et d'indépendance linéaire.
    \item Démonstration qu'une famille de vecteurs est linéairement dépendantes si et seulement si au moins un de ses vecteurs est une combinaison de ceux qui le précèdent.
    \item Démonstration que si le vecteur nul est présent dans la famille de vecteur, ou que si la dimension de nos vecteurs est plus petite que notre nombre de vecteurs, alors ils sont nécessairement linéairement dépendants. 
\end{itemize}

}

\parag{Dans le cas ou le système n'est pas homogène}{
    Il suffit de trouver \important{une} solution de $A \bvec{x} = \bvec{b}$ et \important{toutes} les solutions de $A \bvec{x} = \bvec{0}$ pour avoir \important{toutes} les solutions de $A \bvec{x} = \bvec{b}$. En effet, nous avons le théorème ci-après.
}

\parag{Théorème}{
    Si $\bvec{p}$ est solution de $A \bvec{x} = \bvec{b}$ (en particulier, le système est compatible), alors l'ensemble solution de $A \bvec{x} = \bvec{b}$ est l'ensemble des vecteurs de la forme: 
    \[\bvec{w} = \bvec{p} + \bvec{h}\]
    où $\bvec{h}$ est une solution quelconque de $A \bvec{x} = \bvec{0}$ ($\bvec{h}$ pour homogène).

    En d'autres mots, $\bvec{p}$ est une solution de $A \bvec{x} = \bvec{b}$ et $\bvec{h}$ est une solution quelconque de $A \bvec{x} = \bvec{0}$ si et seulement si $\bvec{w} = \bvec{p} + \bvec{h}$ est une solution de $A \bvec{x} = \bvec{b}$. 
    
   \subparag{Preuve}{
       \important{Première direction:} Si $A \bvec{p} = \bvec{b}$ et $A \bvec{h} = \bvec{0}$, alors
       \[A\left(\bvec{p} + \bvec{h}\right) = A \bvec{p} + A \bvec{h} = \bvec{b} + \bvec{0} = \bvec{b}\]

       Donc, $\bvec{p} + \bvec{h}$ est solution de $A \bvec{x} = \bvec{b}$.

       \vspace{1em}
       \important{Dans l'autre sens:} Si $\bvec{w}$ est une solution de $A \bvec{x} = \bvec{b}$, alors: 
       \[A \bvec{w} = \bvec{b} \text{ et } A\left(\bvec{w} - \bvec{p}\right) = A \bvec{w} - A \bvec{p} = \bvec{b} - \bvec{p} = \bvec{0}\]
       
       Donc, $\bvec{h} = \bvec{w} - \bvec{p}$ est bien une solution de $A \bvec{x} = \bvec{0}$. On n'a donc pas loupé de solution.

       \qed
   }
}

\parag{Interprétation géométrique}{
    Géométrique, l'ensemble solution de $A \bvec{x} = \bvec{b}$ (s'il est non-vide) est une \important{translation} de celui de $A \bvec{x} = \bvec{0}$.

    \imagehere{TranslationSolutionNulle.png}
}

\subsection{Indépendance linéaire}
\parag{Indépendance linéaire}{
    Étant donné une matrice $A$ avec colonnes $\bvec{a}_1, \ldots, \bvec{a}_n$, le système homogène $A \bvec{x} = \bvec{0}$ est équivalent à l'équation vectorielle 
    \[x_1 \bvec{a}_1 + \ldots + x_n \bvec{a}_n = \bvec{0}\]
    
    Demander si $A \bvec{x} = \bvec{0}$ possède une solution non-triviale, est donc équivalent à demander s'il existe une façon non triviale de combiner les vecteurs $\bvec{a}_1, \ldots, \bvec{a}_n$ de façon à obtenir $\bvec{0}$. Si c'est le cas, on dit que les vecteurs sont \important{linéairement dépendants}.

    Plus généralement, on dit qu'une famille $\left(\bvec{v}_1, \ldots, \bvec{v}_p\right)$ de vecteurs de $\mathbb{R}^n$ est \important{libre}, ou que ses vecteurs sont \important{linéairement indépendants}, si l'équation vectorielle 
    \[x_1 \bvec{v}_1 + \ldots + x_p \bvec{v}_p = \bvec{0}\]
    
    admet la solution trivial comme unique solution. À l'inverse, on dit que cette famille est \important{liée}, ou que ses vecteurs sont \important{linéairement dépendants} dans le cas où il existe des coefficients $x_1, \ldots, x_p$ non nuls pour certains tel que l'équation ci-dessus est vérifiée. 

    Les \important{colonnes d'une matrice} $A$ sont linéairement indépendantes si et seulement si l'équation $A \bvec{x} = \bvec{0}$ admet la solution triviale pour solution unique. Donc, étant donnés une famille de vecteurs, on peut vérifier s'ils sont linéairement indépendants ou non en étudiant $A \bvec{x} = \bvec{0}$, avec la matrice $A$ dont les colonnes sont les vecteurs donnés.
}

\parag{Example}{
    Soient 
    \[\bvec{v}_1 = \begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix} \text{ et } \bvec{v}_2 = \begin{bmatrix} 2 \\ 1 \\ 0 \end{bmatrix} \]
    
    On prend donc la matrice 
    \[A = \begin{bmatrix} 1 & 2 \\ 1 & 1 \\ 1 & 0 \end{bmatrix} \]
    
    En la transformant en matrice augentée, puis en matrice échelonnée réduite, et enfin en système, on obtient 
    \begin{systemofequations}{}
    &\ x_1 = 0 \\
    &\ x_2 = 0 \\
    &\ 0 = 0
    \end{systemofequations}

    La seule solution est la solution triviale, donc $\left(\bvec{v}_1, \bvec{v}_2\right)$ est une famille linéairement indépendante.
}

\parag{Le cas d'un unique vecteur}{
    Une famille avec \important{un seul vecteur} $\bvec{v}$ est linéairement indépendant si et seulement si 
    \[\bvec{v} \neq \bvec{0}\]

    Cela se vérifie simplement en regardant la définition de vecteur linéairement dépendant. 
}

\parag{Le cas avec deux vecteurs}{
    Une famille avec \important{deux vecteurs} $\left(\bvec{v}_1, \bvec{v}_2\right)$ est linéairement indépendante si et seulement s'ils ne sont pas colinéaires: 
    \[\nexists c \in \mathbb{R} \telque c \bvec{v}_1 = \bvec{v}_2\]
   
    \subparag{Démonstration}{
        Supposons que $\bvec{v}_1$ et $\bvec{v}_2$ sont linéairement dépendants. Donc, il existe $c_1, c_2 \in \mathbb{R}$ pas tous les deux nuls tels que: 
        \[c_1 \bvec{v}_1 + c_2 \bvec{v}_2 = \bvec{0}\]

        Par hypothèse, il est impossible que $c_1 = c_2 = 0$. Donc séparons notre preuve en deux cas:
        \begin{itemize}[left=1.5cm]
            \item[\important{$c_1 \neq 0$}] On a simplement $\bvec{v}_1 = -\frac{c_2}{c_1} \bvec{v}_2$.
            \item[\important{$c_2 \neq 0$}] On a que $\bvec{v}_2 = -\frac{c_1}{c_2} \bvec{v}_1$.
        \end{itemize}

        Dans les deux cas, les vecteurs peuvent être écrits comme un multiple l'un de l'autre, donc ils sont colinéaires. Il n'y a pas d'autres cas possible.

        \qed
    }
}

\parag{Illustration pour deux vecteurs}{
    \imagehere{ExempleDependanceLineaire.png}

    Dans le cas où on a 
    \[\bvec{v}_1 = \begin{bmatrix} 2 \\ 1 \end{bmatrix}, \mathspace \bvec{v}_2 = \begin{bmatrix} -4 \\ -2 \end{bmatrix} \]
    
    Ils sont colinéaires, donc ils sont linéairement dépendants. En effet, 
    \[1 \bvec{v}_2 + 2\bvec{v}_1 = \bvec{0}\]
    
    Cependant, si on a 
    \[\bvec{v}_1 = \begin{bmatrix} 2 \\ 1 \end{bmatrix}, \mathspace \bvec{v}_2 = \begin{bmatrix} 0 \\ 1 \end{bmatrix}\]

    Alors, $x_1 \bvec{v}_1 + x_2 \bvec{v}_2 = \bvec{0}$ n'a que la solution $x_1 = x_2 = 0$. Géométriquement, il est clair que si on se balade le long de $\bvec{v}_1$, puis qu'on se balade le long de $\bvec{v}_2$, il est impossible d'atteindre $\bvec{0}$ (sauf si on ne bouge pas du centre).
}

\parag{Trois vecteurs}{
    Peut-on avoir trois vecteurs dans $\mathbb{R}^3$ tels que aucune paire n'est colinéaire, mais les vecteurs sont quand même linéairement dépendants ? Oui, par exemple: 
    \[\bvec{v}_1 = \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}, \mathspace \bvec{v}_2 = \begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix}, \mathspace \bvec{v}_3 = \begin{bmatrix} 1 \\ 1 \\ 0 \end{bmatrix} \]
    
    Aucune paire de vecteurs n'est colinéaire, cependant 
    \[\bvec{v}_1 + \bvec{v}_2 - \bvec{v}_3 = \bvec{0}\]

    Ces trois vecteurs sont tout de même un peu spéciaux, puisque $\vect\left(\bvec{v}_1, \bvec{v}_2, \bvec{v}_3\right)$ ne donne pas $\mathbb{R}^3$, mais un plan. Cela découle directement du fait qu'ils ne soient pas linéairement indépendants.
}

\parag{Théorème}{
    Si $\left(\bvec{v}_1, \ldots, \bvec{v}_p\right)$ sont linéairement dépendants, alors au moins un des vecteurs est une \important{combinaison linéaire de ceux qui le précèdent}. Cela généralise ce qu'on a dit pour deux vecteurs (qu'il faut qu'ils soient colinéaires pour être linéairement indépendants).

    \subparag{Exemple}{
     Si on reprend nos vecteurs ci-dessus:
    \[\bvec{v}_1 = \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}, \mathspace \bvec{v}_2 = \begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix}, \mathspace \bvec{v}_3 = \begin{bmatrix} 1 \\ 1 \\ 0 \end{bmatrix} \]
     Ils sont linéairement dépendants. Le vecteur $\bvec{v}_1$ est précédé par ``rien'' qui, par convention, est le vecteur nul. Or, il est impossible d'écrire $\bvec{v}_1$ sous la forme d'une combinaison linéaire du vecteur nul.

     De la même manière, il est impossible d'écrire $\bvec{v}_2$ comme une combinaison linéaire du vecteur $\bvec{v}_1$.

     Cependant, il est possible d'écrire le troisième vecteur comme une combinaison linéaire de $\bvec{v}_1$ et $\bvec{v}_2$: 
     \[\bvec{v}_3 = \bvec{v}_1 + \bvec{v}_2\]
    }
    

    \subparag{Preuve}{
        Puisque les vecteurs sont linéairement indépendants, il existe $c_1, \ldots, c_p$ non tous nuls (il en existe au moins un qui n'est pas nul) tels que 
        \[c_1 \bvec{v}_1 + \ldots + c_p \bvec{v}_p = \bvec{0}\]
        
        \important{Si $c_p \neq 0$}, alors on peut écrire 
        \[c_p \bvec{v}_p = -c_1 \bvec{v}_1 - \ldots - c_{p-1} \bvec{v}_{p-1} \]

        Puisque $c_p \neq 0$, on peut diviser par ce nombre: 
        \[\bvec{v}_p = \frac{-c_1}{c_p}\bvec{v}_1 - \ldots - \frac{c_{p-1}}{c_p}\bvec{v}_{p-1}\]
        
        Ce qui montre que $\bvec{v}_p$ est combinaison linéaire de $\bvec{v}_1, \ldots, \bvec{v}_{p-1}$, comme souhaité.

        \vspace{1em}
        \important{Si $c_p = 0$}, alors on a 
        \[c_1 \bvec{v}_1 + \ldots + c_{p-1} \bvec{v}_{p - 1} = \bvec{0}\]
        
        On est de retour dans notre situation de départ, donc on peut donc simplement recommencer le raisonnement avec $c_{p-1}$. Dans le cas où il est encore nul, on recommence avec $c_{p-2}$, etc. Vu qu'il y a au moins un coefficient qui est non-nul, on va forcément en avoir un à un moment. 
    }

    La réciproque de ce théorème est aussi juste: 

    \subparag{Réciproque}{
        Si au moins un des vecteurs $\bvec{v}_1, \ldots, \bvec{v}_p$ est une combinaison linéaire de ceux qui le précédent, alors $\bvec{v}_1, \ldots, \bvec{v}_p$ sont linéairement dépendants.
    }

    Sa démonstration est plus simple, mais elle est trouvable dans les slides du prof' si nécessaire.
    
   \subparag{Conséquence}{
       \imagehere{DependenceLineaireAppartientAVect.png}

       Supposons que $\bvec{v}_1, \ldots, \bvec{v}_p$ sont linéairement indépendants. Étant donné un vecteur $\bvec{w}$, les vecteurs $\bvec{v}_1, \ldots, \bvec{v}_p, \bvec{w}$ sont linéairement dépendants \important{si et seulement si} $\bvec{w}$ est une combinaison linéaire de $\bvec{v}_1, \ldots, \bvec{v}_p$. 

       Autrement dit, $\bvec{v}_1, \ldots, \bvec{v}_p, \bvec{w}$ sont linéairement dépendants \important{si et seulement si} $\bvec{w} \in \vect\left(\bvec{v}_1, \ldots, \bvec{v}_p\right)$.

       Cela se démontre facilement en utilisant le théorème vu ci-dessus. $\bvec{v}_i$ ne peut pas être une combinaison linéaire des vecteurs qui le précèdent, pour n'importe quel $i$, de là on en déduit que ça doit être $\bvec{w}$ qui est dépendant de ces derniers. Ce n'est qu'un sens de l'implication, mais ça donne l'idée de comment démontrer ce théorème.
   }
    
}


\parag{Théorème}{
    Si un des vecteurs de $\left(\bvec{v}_1, \ldots, \bvec{v}_p\right)$ est nul, alors la famille est linéairement dépendante.

    \subparag{Preuve}{
        Si $\bvec{v}_j = \bvec{0}$ pour un certain $j$, alors on peut choisir $c_j = 1$ (qui n'est pas nul!), $c_1 = \ldots = c_{j-1} = 0$ et $c_{j+1} = \ldots = c_p = 0$ (tous nuls sauf $c_j$), alors 
        \[\underbrace{c_1 \bvec{v}_1 + \ldots}_{\bvec{0}} + \underbrace{c_j \bvec{v}_j}_{1\cdot\bvec{0}} + \underbrace{\ldots + c_p \bvec{v}_p}_{\bvec{0}} = \bvec{0}\]
        \qed
    }
    
}

\parag{Théorème}{
    Une famille $\left(\bvec{v}_1, \ldots, \bvec{v}_p\right)$ de vecteurs dans $\mathbb{R}^{n}$ est nécessairement linéairement dépendante si $p > n$ (si $p \leq n$, alors tout peut arriver).

    \subparag{Preuve}{
        Soit 
        \[A = \begin{bmatrix}  &  &  \\ \bvec{v}_1 & \ldots & \bvec{v}_p \\  &  &  \end{bmatrix} \in \mathbb{R}^{n \times p}\]

        Les colonnes sont linéairement dépendantes si (et seulement si) le système $A \bvec{x} = \bvec{0}$ a une solution non-triviale. On sait que le système est compatible (car $\bvec{0}$ est solution). De plus, il y a seulement $n$ équations pour $p > n$ variables ; donc, au moins une des variables est libre. Puisqu'il ne peut peut pas y avoir de contradiction (le système est compatible), cela implique que $A \bvec{x} = \bvec{0}$ a une infinité de solutions.
        \qed
    }
    
}




\end{document}
