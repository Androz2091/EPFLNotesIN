\documentclass[a4paper]{article}

% Expanded on 2022-02-22 at 15:14:58.

\usepackage{../../style}

\title{AICC-2}
\author{Joachim Favre}
\date{Mardi 22 f√©vrier 2022}

\begin{document}
\maketitle

\lecture{1}{2022-02-22}{Back to probabilities}{
\begin{itemize}[left=0pt]
    \item Introduction to the different subjects about which we will discuss this semester.
    \item Recall of discrete probabilities, i.e. of conditional probabilities, independent events, disjoint events, Bayes' rule, random variables, and expected values.
\end{itemize}
}

\section{Introduction}
\parag{Introduction}{
    AICC-I was about computation, now AICC-II is about communication.
}

\parag{Subjects}{
    Point-to-point communication system usually work this way:

    \svgherenolatex[0.7]{PointToPointCommunication.svg}

    Source coding consists in compressing the data so that we can send it. It will rely on discrete probability theory; which was done by many people, including Shannon (he is as important for AICC II as Turing was for AICC I).

    Cryptography consists in hiding the data, so that nobody with bad intentions can get it on the way. It will rely on number theory (a lot of work from Euler and Fermat) and group theory.

    Channel coding consists in turning the data in a way that can be used by the physical channel to transmit it. This will rely on finite fields (from Galois) and linear algebra (from Shannon).
}

\section{Source coding}
\subsection{Discrete probabilities}
\parag{Definitions}{
    The \important{sample space}, $\Omega = \left\{\omega_1, \ldots, \omega_n\right\}$, is the set of all possible outcomes. An \important{event} $E$ is a subset of $\Omega$.
}

\parag{Probability of an event}{
    Let's say that all our events are equally likely. Then, the probability of an event is easily computed:
    \[p\left(E\right) = \frac{\left|E\right|}{\left|\Omega\right|}\]

    Else, we need to define the \important{probability distribution} (probability mass function) $p$ to be a function $p: \Omega \mapsto \left[0, 1\right]$ such that:
    \[\sum_{\omega \in \Omega}^{} p\left(\omega\right) = 1\]

    Then, the probability of an event can be computed the following way:
    \[p\left(E\right) = \sum_{\omega \in E}^{} p\left(\omega\right)\]

}

\parag{Conditional probability}{
    We want to know what is the probability that $E$ occurs, knowing that the event $F$ has occurred (hence assuming that $p\left(F\right) \neq 0$).

    Let's first consider the case where every event has the same probability. This is rather easy since $F$ is basically defining a new sample space, so:
    \[p\left(E | F\right) = \frac{p\left(E \cap F\right)}{p\left(F\right)}= \frac{\left|E \cap F\right|}{\left|F\right|}\]

    This gets generalised in a rational way:
    \[p\left(E | F\right) = \frac{p\left(E \cap F\right)}{p\left(F\right)}\]

    \svghere[0.5]{ConditionalProbability.svg}
}

\parag{Example}{
    Let's say we know that a family with two children has one girl. We wonder what is the probability that the other child is a girl.

    Let's define the sample space, where every event is equally likely:
    \[\Omega = \left\{GG, GB, BG, BB\right\}\]

    Now, we know that one of the children is a girl, so:
    \[F = \left\{GG, GB, BG\right\}\]

    And, we want to know when the other child is a girl, so:
    \[E = \left\{GG\right\}\]

    We can now compute the probability:
    \[p\left(E | F\right) = \frac{p\left(E \cap F\right)}{p\left(F\right)} = \frac{1}{3}\]

    \subparag{Intuition}{
        If we knew that the first child is a girl, the probability that the second child is a girl is $\frac{1}{2}$. Note that there is a difference in the problem which we just solve: we know that \textit{one of the children} is a girl.

        The Professor says that the human brain is not very good at dealing with probabilities.
    }
}

\parag{Independent events}{
    We say that two event $E$ and $F$ are \important{independent} if and only if:
    \[p\left(E | F\right) = p\left(E\right)\]

    In other words, knowing that $F$ happened does not bring any information to $E$.
}


\parag{Disjoint events}{
    We say that two events $E_1$ and $E_2$ are \important{disjoint} if $E_1 \cap E_2 = \o$. Then, we have the following property:
    \[p\left(E_1 \cup E_2\right) = p\left(E_1\right) + p\left(E_2\right)\]

    We note that, if two events are disjoint, then there are definitely not independent. Indeed, knowing that an event happened means that the other event did not happen.
}

\parag{Law of Total probability}{
    For any event $F \subseteq \Omega$ and its complement $F^C$, we have the following equivalence:
    \[p\left(E\right) = p\left(E|F\right)p\left(F\right) + p\left(E|F^C\right)p\left(F^C\right)\]

    More generally, if $\Omega$ is the union of disjoint event $F_1, \ldots, F_n$, then:
    \[p\left(E\right) = p\left(E|F_1\right)p\left(F_1\right) + \ldots + p\left(E|F_n\right)p\left(F_n\right)\]


    \subparag{Proof}{
        We can split out event $E$ as:
        \[E = \left(E \cap F\right) \cup \left(E \cap F^C\right)\]


        \svghere[0.6]{EventSplittedComplement.svg}

        We notice that $E \cap F$ and $E \cap F^C$ are disjoint by construction: $F$ and $F^C$ are disjoint. We can use our property for disjoint event:
        \[p\left(E\right) = p\left(\left(E \cap F\right) \cup \left(E \cap F^C\right)\right) = p\left(E \cap F\right) + p\left(E \cap F^C\right)\]

        Let us now multiply and divide by a quantity (thus multiplying by 1):
        \begin{multiequality}
            p\left(E\right) =\ & \frac{p\left(E \cap F\right)}{p\left(F\right)}p\left(F\right) + \frac{p\left(E \cap F^C\right)}{p\left(F^C\right)} p\left(F^C\right)  \\
            =\ & p\left(E | F\right)p\left(F\right) + p\left(E | F^C\right)p\left(F^C\right)
        \end{multiequality}

        \qed
    }

    \subparag{Personal note}{
        This result allows us to draw trees for our problems:
        \svghere[0.7]{LawOfTotalProbabilityTree.svg}
    }

}

\parag{Bayes' Rule}{
    Let's say we know $p\left(E\right), p\left(F\right)$ and $p\left(E | F\right)$, and we want $p\left(F | E\right)$. We can use the following relation:
    \[p\left(F | E\right) = \frac{p\left(E |F\right)p\left(F\right)}{p\left(E\right)}\]

    \subparag{Proof}{
        We can use the definition of conditional probability:
        \[p\left(E |F\right) = \frac{p\left(E \cap F\right)}{p\left(F\right)} \iff p\left(E \cap F\right) = p\left(E | F\right)p\left(F\right)\]

        We can do the same trick with $p\left(F | E\right)$ to get that:
        \[p\left(F |E\right) = \frac{p\left(F \cap E\right)}{p\left(E\right) } \iff p\left(F \cap E\right) = p\left(F | E\right)p\left(E\right)\]

        Thus, since $p\left(E \cap F\right) = p\left(F \cap E\right)$:
        \[p\left(E |F\right) p\left(F\right) = p\left(F | E\right) p\left(E\right) \iff p\left(F | E\right) = \frac{p\left(E |F\right)p\left(F\right)}{p\left(E\right)}\]

    }

}

\parag{Random variable}{
    We define \important{random variables} to be functions $X : \Omega \mapsto \mathbb{R}$.

    In other words, they map every event to a real number.
}

\parag{Probability distribution of a random variable}{
    $p_X\left(x\right)$ is the probability that $X = x$, i.e $p\left(E\right)$ where:
    \[E = \left\{\omega \in \Omega : X\left(\omega\right) = x\right\}\]

    We know how to compute such events, so we can compute $p_X\left(x\right)$:
    \[p_X\left(x\right) = \sum_{\omega \in E}^{} p\left(\omega\right)\]

    \subparag{Personal note: Notations}{
        Note that we also sometimes write $p_X\left(x\right) = p\left(X = x\right)$. Later, we will also see:
        \[p_{X|Y}\left(x, y\right) = p\left(X = x | Y = y\right)\]

        Also, when everything is really clear and we are only redondent, we can omit the subscript:
        \[p_{X}\left(x\right) = p\left(x\right), \mathspace p_{X|Y}\left(x| y\right) = p\left(x|y\right)\]
    }

}

\parag{Intervals}{
    We wonder what is the probability that $X \in \left[a, b\right]$. We can compute it in the following way:
    \[\sum_{\omega \in G}^{} p\left(\omega\right), \mathspace \text{where } G = \left\{\omega \in \Omega : X\left(\omega\right) \in \left[a, b\right]\right\}\]

    We can also do it using $p_X$:
    \[\sum_{x \in\left[a, b\right]}^{} p_X\left(x\right)\]
}

\parag{Two random variables}{
    Let $X : \Omega \mapsto \mathbb{R}$ and $Y : \Omega \mapsto \mathbb{R}$ be two random variables. In other words, we map every event to two real lines.

    We want to find the probability of an event $E_{\left(x, y\right)} = \left\{\omega \in \Omega : X\left(\omega\right) = x \text{ and } Y\left(\omega\right) = y\right\}$, denoted $p_{X, Y}\left(x, y\right)$:
    \[p_{X, Y}\left(x, y\right) = \sum_{\omega \in E_{\left(x, y\right)}}^{} p\left(\omega\right)\]

    We can compute $p_X$ from $p_{X, Y}$:
    \[p_X\left(x\right) = \sum_{y}^{} p_{X, Y}\left(x, y\right)\]

    We call $p_X$ the \important{marginal distribution} of $p_{X, Y}\left(x, y\right)$ with respect to $x$. $p_Y$ can be computed similarly.

    \subparag{Personal note: Notations}{
        Note that, sometimes we write:
        \[p_{X, Y}\left(x, y\right) = p\left(X = x \cap Y = y\right) = p\left(X = x \land Y = y\right)\]
    }
}

\parag{Example}{
    Let's say we throw two dice, compute its sum and write it as a two digit number $L = L_1 L_2$, where $L_1 \in \left\{0, 1\right\}$ and $L_2 \in \left\{0, 1, \ldots, 9\right\}$ are random variables.

    We can draw the following table:
    \begin{center}
    \begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c}
        $L$ & 02 & 03 & 04 & 05 & 06 & 07 & 08 & 09 & 10 & 11 & 12  \\
        \hline
        $p_L$ & $\frac{1}{36}$ & $\frac{2}{36}$ & $\frac{3}{36}$ & $\frac{4}{36}$ & $\frac{5}{36}$ & $\frac{6}{36}$ & $\frac{5}{36}$ & $\frac{4}{36}$ & $\frac{3}{36}$ & $\frac{2}{36}$ & $\frac{1}{36}$
    \end{tabular}
    \end{center}

    We can marginalise to get the marginal distribution of $p_L$ with respect to $L_1$:
    \[p_{L_1}\left(1\right) = \sum_{x}^{} p_{L_1, L_2}\left(1, x\right) = \frac{3}{36} + \frac{2}{36} + \frac{1}{36} = \frac{1}{6}\]

    We can do a similar computation to get that $p_{L_1}\left(0\right) = \frac{5}{6}$.
}

\parag{Expected value}{
    The \important{Expected Value} $\exval\left[X\right]$ of a random variable $X : \Omega \mapsto \mathbb{R}$ is given by:
    \[\exval\left[X\right] = \sum_{\omega}^{} X\left(\omega\right)p\left(\omega\right) \]

     This is basically a weighted mean.
}

\parag{Theorem}{
    We can compute an expected value of a random variable $X : \Omega \mapsto \mathbb{R}$ the following way:
    \[\exval\left[X\right] = \sum_{x}^{} xp_X\left(x\right)\]

    \subparag{Proof}{
        Let's begin with our definition, and combine terms:
        \[\exval\left[X\right] = \sum_{\omega}^{} X\left(\omega\right)p\left(\omega\right) = \sum_{x}^{} \left(\sum_{\omega : X\left(\omega\right) = x}^{} X\left(\omega\right) p\left(\omega\right)\right)\]

        Since we grouped terms such that $X\left(\omega\right) = x$, we have:
        \[\exval\left[X\right] = \sum_{x}^{} \sum_{\omega : X\left(\omega\right) = x}^{} x p\left(\omega\right) = \sum_{x}^{} x\sum_{\omega : X\left(\omega\right) = x}^{} p\left(\omega\right)\]

        We can know recognise the definition of probability distribution of random variable, so:
        \[\exval\left[X\right] = \sum_{x}^{} x p_X\left(X\right)\]
    }
}


\parag{Theorem: linearity of expectations}{
    Expectation is a linear operation. In other words, let $X_1, \ldots, X_n$ be random variables and $\alpha_1, \ldots, \alpha_n$ be scalars. Then:
    \[\exval\left[\sum_{i=1}^{n} \alpha_i X_i\right] = \sum_{i=1}^{n} \alpha_i \mathbb{E}\left[X_i\right]\]

    Note that random variables are functions, so it makes sense to add them up, whereas it would never have made sense to add up events.

    \subparag{Proof}{
        Let's begin with the definition:
        \[\exval\left[\sum_{i=1}^{n} \alpha_i X_i\right] = \sum_{\omega}^{} p\left(\omega\right) \left[\sum_{i=1}^{n} X_i\left(\omega\right) \alpha_i\right] = \sum_{\omega}^{} \sum_{i=1}^{n} p\left(\omega\right) X_i\left(\omega\right) \alpha_i\]

        We can swap the two sums:
        \[\sum_{i=1}^{n} \sum_{\omega}^{} p\left(\omega\right)X_i\left(\omega\right)\alpha_i = \sum_{i=1}^{n} \alpha_i \sum_{\omega}^{} p\left(\omega\right)X_i = \sum_{i=1}^{n} \alpha_i \exval\left[X_i\right]\]
        by definition of the expected value of $X_i$.
    }

}




\end{document}
