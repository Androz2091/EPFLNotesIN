\documentclass[a4paper]{article}

% Expanded on 2022-02-24 at 08:18:12.

\usepackage{../../style}

\title{AICC 2}
\author{Joachim Favre}
\date{Jeudi 24 fÃ©vrier 2022}

\begin{document}
\maketitle

\lecture{2}{2022-02-24}{IT inequality}{
\begin{itemize}[left=0pt]
    \item Definition of independent random variables.
    \item Definition of entropy, and proof of its bounds.
    \item Explanation of the IT-inequality.
\end{itemize}
}

\parag{Example: Hat party}{
    We have $n$ men arriving at a party and leaving their hat. When they leave, they take a random hat. We wonder what is the expected value of men getting their own hat back.

    Let's define the following random variable: 
    \begin{functionbypart}{R_i}
    1, \mathspace \text{man $i$ gets his own hat} \\
    0, \mathspace \text{man $i$ does not get his own hat}
    \end{functionbypart}
    
    We thus want the following expected value: 
    \[\exval\left[R\right] = \exval\left[R_1 + R_2 + \ldots + R_n\right] = \exval\left[R_1\right] + \ldots + \exval\left[R_n\right]\]
    
    So we only need to compute one of those expected values: 
    \[\exval\left[R_i\right] = 1 \cdot P\left(\text{right}\right) + 0\cdot P\left(\text{wrong}\right) = 1 \cdot \frac{1}{n} = \frac{1}{n}\]
    
    Thus, we get that: 
    \[\exval\left[R\right] = n \cdot \frac{1}{n} = 1\]

    \subparag{Independence}{
        We can clearly see that the $R_i$ are not independent. For example, if there are only two men, the second men has the same fate as the first one.
    }
}

\parag{Definition: Independent random variables}{
    Two random variables $X$ and $Y$ are \important{independent} if and only if, for all realisations $x$ and $y$: 
    \[p\left(\left\{X = x\right\} \cap \left\{Y = y\right\}\right) = p\left(\left\{X = x\right\}\right) p\left(\left\{Y = y\right\}\right)\]
    
    More concisely, if and only if:
    \[p_{X, Y}\left(x, y\right) = p_X\left(x\right)p_Y\left(y\right) \]
    
    We can generalise this to $n$ random variables. The random variables $X_1, \ldots, X_n$ are independent if and only if: 
    \[p_{X_1, \ldots, X_n}\left(x_1, \ldots, x_n\right) = \prod_{i=1}^{n} p_{X_i}\left(x_i\right)\]

    \subparag{Link with earlier definition}{
        We defined that two event $E$ and $F$ are independent if and only if: 
        \[p\left(E \cap F\right) = p\left(E\right)p\left(F\right)\]
    }
    
    \subparag{Utility}{
        It is very useful to know that two random variables are independent in medical statistics, for example.
    }
}

\parag{Definition: Conditional distribution}{
    The \important{conditional distribution} of $Y$ given $X$ is the function $p_{Y | X}$ defined by: 
    \[p_{Y|X}\left(y | x\right) = \frac{p_{X, Y}\left(x, y\right)}{p_{X\left(x\right)}}\]

    \subparag{Personal note: Notations}{
        As mentioned in the last course, sometimes we write: 
        \[p_{Y|X}\left(y|x\right) = p\left(Y = y | X = x\right)\]
        
        Also, sometimes we remove subscripts not to have redundancy: 
        \[p_{Y|X}\left(y|x\right) = p\left(x|y\right)\]
        
    }
}

\parag{Independence}{
    All the following sentences are equivalent:
    \begin{itemize}
        \item $X$ and $Y$ are independent.
        \item $\displaystyle p_{X, Y} = p_{X} p_Y$
        \item $\displaystyle p_{Y | X}\left(y | x\right) = p_Y\left(y\right)$ 
        \item $\displaystyle p_{X | Y}\left(x|y\right) = p_{X}\left(x\right)$
    \end{itemize}
}

\parag{Back to the example with the dice}{
    Let's go back to the example in which we throw two dice, and write its sum as a two digit number.

    We now wonder whether $L_1$ and $L_2$ are independent. We can find a counterexample: 
    \[p_{L}\left(13\right) = 0 \neq p_{L_1}\left(1\right) p_{L_2}\left(3\right) = \frac{1}{6}\cdot \frac{3}{36}\]
}

\parag{Product of expected values}{
    If $X$ and $Y$ are independent, then: 
    \[\exval\left[XY\right] = \exval\left[X\right] \exval\left[Y\right]\]
    
    \subparag{Hypothesis}{
        The hypothesis is important. Indeed, let $X = Y \in \left\{-1, 1\right\}$ uniformly distributed. They are clearly not independent (since they are equal), and the formula does not hold: 
        \[\exval\left[XY\right] = 1 \neq 0 = \exval\left[X\right]\exval\left[Y\right]\]
    }

    \subparag{Advice}{
        When proving or disproving dependence, we must not hesitate to use random variables that are equal, or that give impossible results. Such edge cases can be easy to compute.
    }
}

\subsection{Entropy}
\parag{Information}{
    We communicate by revealing the value of a sequence of variables that we call (information) symbols.

    The $i$\Th symbol might represent the intensity of the $i$\Th pixel of black and white digital photo, a score at our $i$\Th exam, the $i$\Th letter of a text, and so on.

    In an article that appeared in 1928, Hartley wrote: ``A symbol provides information only if there had been other possibilities for its value, besides that which was revealed''. In modern language, Hartley was saying that the value of a symbol provides information only if the symbol is a (non-constant) random variable.
}

\parag{Example}{
    We now want to quantify the quantity of information we get.

    Let's take the following sample space: 
    \[W = \left\{\text{sun}, \text{rain}, \text{clouds}, \text{snow}, \text{hail}\right\}\]
    
    We could say that the information in $W$ is the number of possible values it could have taken. The more information there would have been, the more intuitive the random variable should be. In this case, it would be 5.

    We do not want to stop there. If we look through the window in the morning and the day after. Then there are 25 possibilities, so the information will be 25. It seems really weird to take the product, and we would like to instead have an addition. Let's thus take a logarithm. Thus, the entropy of $W$ would be $\log\left(5\right)$.
}

\parag{Example}{
    The world population in 2022 is estimated to be 7.954 billion. Hence, we need $\log_2\left(7.954 \cdot 10^{9}\right) = 32.9$ bits of information to identify a person. 

    The world population in 1970 is estimated to have been 3.7 billion. We know that $\log_2\left(\frac{x}{2}\right) = \log_2\left(x\right) - 1$. So, around 32 bits of information were needed at this time.
}

\parag{Problem}{
    There is still a problem with this definition. 

    Let's suppose that $S_n \in \left\{\text{sunny}, \text{rainy}\right\}$ is the weather prognosis for day $n+1$, revealed on day $n$. Let's suppose that $S_n = \text{rainy}$ has probability $\frac{5}{365}$.

    It seems intuitively obvious that the amount of information provided by $S_n = \text{runny}$ is much higher than that provided by $S_n = \text{sunny}$. However, our current definition assigns $\log_2\left(2\right) = 1$ bit of information to both.
}

\parag{Entropy}{
    We finally come up with the definition of \important{entropy}, the one Shannon defined: 
    \[H_b\left(S\right) = - \sum_{s \in \supp\left(p_S\right)}^{} p_S\left(s\right) \log_b\left(p_S\left(s\right)\right), \mathspace \text{where } \supp\left(p_S\right) = \left\{s: p_S\left(s\right) > 0\right\}\]

    We need the $\supp\left(p_S\right)$, the support of $p_S$, since $\log\left(0\right)$ is undefound (mistake made on purpose, we have to be coherent with our incoherences). To simplify the notation, we can also declare that $p_S\left(s\right) \log\left(p_S\left(s\right)\right) = 0$ when $p_S\left(s\right) = 0$ (this is coherent since $x \log\left(x\right) \to 0$ when $x \to 0$). This convention allows us to simplify the notation to: 
    \[H_b\left(S\right) = -\sum_{s \in A}^{} p_S\left(s\right) \log_b p_S\left(s\right)\]
    
    We can also see it the following way: 
    \[H\left(S\right) = \exval\left[-\log\left(p_S\left(S\right)\right)\right]\]

    \subparag{Unit}{
        The choice of the base $b$ determines the unit. Typically, $b = 2$ leads to bits, and much less common, $b = e$ leads to nats.
    }
    
    \subparag{Graph}{
        We can plot $f\left(x\right) = x \log\left(x\right)$:
        \svghere[0.7]{EntropyGraph.svg}
    }

    \subparag{Intuition}{
        The concept of entropy might be a bit unintuitive; what is really entropy? We will answer this question throughout this semester during this course, seeing that it comes up for many algorithms.

        For now, we can see it a value representing how random something is. We will see that the entropy of a random variable is bounded above by the one of the uniform distribution (clearly the distribution which gives the most random results) and bounded below by certainty (there is 100\% chance of something happening). 
    }
    
}

\parag{Uniform distribution}{
    Let $p_S\left(s\right)$ be a uniform distribution, i.e.: 
    \[p_S\left(s\right) = \frac{1}{\left|A\right|}\]
    
    Then,we can compute the entropy: 
    \[H_b\left(S\right) = -\sum_{s \in A}^{} \frac{1}{\left|A\right|} \log_b\left(\frac{1}{\left|A\right|}\right) = \log_b\left|A\right| \underbrace{\sum_{s \in A}^{} \frac{1}{\left|A\right|}}_{= 1} = \log_b \left|A\right|\]

    Shanon's proposal and Harley's proposal are thus equivalent on uniform distribution.
}

\parag{Example 2}{
    Let's say that a monkey produces text by selecting letters at random from a French text. We can use French's letter distribution. 
    \[H \approx \SI{3.9425}{\bit}\]
    
    Note that the maximum entropy of a source with $\left|A\right| = 26$ is: 
    \[\log_2\left(26\right) \approx \SI{4.7004}{\bit}\]
}

\parag{Binary entropy function}{
    Let's consider the special case when $\left|A\right| = 2$. 

    In this case, $p_S$ has only two possible values, let's say $p$ and $\left(1 - p\right)$. This corresponds to the entropy: 
    \[H\left(S\right) = h\left(p\right) = -p\log_2\left(p\right) - \left(1 - p\right) \log_2\left(1 - p\right)\]

    We can draw the function:
    \svghere[0.5]{BinaryEntropyFunction.svg}

    It is extremely steep at 0 and 1, it is symmetric around $\frac{1}{2}$, and reaches its maximum at $x = \frac{1}{2}$.

    We call $h\left(p\right)$ the \important{binary entropy function}.
}

\parag{Example}{
    Let's say Anne has a random lock number. Let $S$ be the answer to the question ``Is 8950 Anne's lock number?'', i.e. $S \in A = \left\{YES, NO\right\}$. Its entropy is given by: 
    \[h\left(\frac{1}{10^4}\right) \approx \SI{0.001}{\bit}\]
}

\parag{Lemma: IT-inequality}{
    Let $r \in \mathbb{R}_+$. Then: 
    \[\log_b\left(r\right) \leq \left(r - 1\right) \log_b\left(e\right)\]
    with equality if and only if $r = 1$.
    
    \subparag{Proof}{
        We can get some intuition using the following graph:
        \svghere[0.8]{ITInequalityGraph.svg}

        We can make a formal proof using calculus tools.
    }

    \subparag{Importance}{
        This inequality is really important in Computer Science; many results will result from it.
    }

    \subparag{Personal note}{
        To remember this inequality, we only need to remember that we are looking for the tangent line at $r = 1$. We can compute the derivative quickly, which gives us the coefficient $\log_b\left(e\right)$.
    }
}

\parag{Theorem: Entropy bound}{
    The entropy of a discrete random variable $S \in A$ satisfies: 
    \[0 \leq H_b\left(S\right) \leq \log_b\left|A\right|\]
    with equality on the left if and only if $p_S\left(s\right) = 1$ for some $s$ (certainty), an with equality on the right if and only if $p_S\left(s\right) = \frac{1}{\left|A\right|}$ for all $s$ (uniform distribution).

    \subparag{Proof of left inequality}{
        We can observe that:
        \begin{functionbypart}{-p_S\left(s\right) \log\left(p_S\left(s\right)\right)}
            0, \mathspace \text{if } p_S\left(s\right) \in \left\{0, 1\right\} \\
            > 0, \mathspace \text{if } 0 < p_S\left(s\right) < 1
        \end{functionbypart}

        Since we are only adding non-negative values, we get that $H\left(S\right) \geq 0$. Also, the only way to get 0 is to stay in the first case, thus $p_S\left(s\right)$ must always either equal 0 or 1. Since it must add up to 1, then $p_S\left(s\right) = 1$ for some $s$.
    }

    \subparag{Proof of right inequality}{
        We want to prove that: 
        \[H_2\left(s\right) - \log_2 \left|A\right| \leq 0\]

        By definition of the entropy, and multiplying by 1:
        \begin{multiequality}
        H_2\left(s\right) - \log_2\left|A\right| =\ & - \sum_{s}^{} p\left(s\right)\log\left(p\left(s\right)\right) - \underbrace{\sum_{s}^{} p\left(s\right)}_{= 1} \log_2 \left|A\right|  \\
        =\ &  \sum_{s}^{} p\left(s\right) \left[\log\left(\frac{1}{p\left(s\right) \left|A\right|}\right)\right] \\
        \over{\leq}{IT} \ & \sum_{s}^{} p\left(s\right)\left[\frac{1}{p\left(s\right) \left|A\right|} - 1\right] \log_2\left(e\right) \\
        =\ & \sum_{s}^{} \left[\frac{1}{\left|A\right|} - p\left(s\right)\right] \log_2\left(e\right)  \\
        =\ & \left[\sum_{s}^{} \frac{1}{\left|A\right|} - \sum_{s}^{} p\left(s\right)\right] \log_2\left(e\right)  \\
        =\ & \left(1 - 1\right)\log_2\left(e\right) \\
        =\ & 0 
        \end{multiequality}
        
        Taking back the inequality:
        \[\sum_{s}^{} p\left(s\right) \left[\log\left(\frac{1}{p\left(s\right) \left|A\right|}\right)\right] \leq \sum_{s}^{p\left(s\right)} \left[\frac{1}{p\left(s\right) \left|A\right|} - 1\right] \log_2\left(e\right)\]
        
        We know that this is the IT-inequality, and thus that it is equal if and only if $r = 1$, i.e: 
        \[\frac{1}{p\left(s\right) \left|A\right|} = 1 \iff p\left(s\right) = \frac{1}{\left|A\right|}\]
    }
}

\parag{Two random variables}{
    We can also define entropy for two random variables: 
    \[H\left(X, Y\right) = -\sum_{\left(x, y\right) \in \mathcal{X} \times \mathcal{Y}}^{} p_{X,Y}\left(x, y\right) \log\left(p_{X,Y}\left(x,y\right)\right)\]

    We can also define it using expected values: 
    \[H\left(X, Y\right) = \exval\left[-\log\left(p_{X,Y}\left(X, Y\right)\right)\right]\]
}

\parag{Example}{
    Let's say that $p_{X,Y}$ is given by the following table:
    \begin{center}
    \begin{tabular}{c|c|c}
        $x$ & $y$ & $p_{X,Y}\left(x, y\right)$ \\
        \hline
        0 & 0 & $1 / 8$ \\
        0 & 1 & $3 / 8$ \\
        1 & 0 & $1 / 4$ \\
        1 & 1 & $1 / 4$ \\
    \end{tabular}
    \end{center}

    Then, we can compute the entropy:
    \[H\left(X, Y\right) = -\frac{1}{8} \log_2\left(\frac{1}{8}\right) - \frac{3}{8} \log_2\left(\frac{3}{8}\right) - \frac{1}{4}\log_2\left(\frac{1}{4}\right) - \frac{1}{4}\log_2\left(\frac{1}{4}\right)\]
}

\parag{Sequences of random variables}{
    We want to compress large amounts of data, thus we can have long sequences of random variables. It can be finite, $\left(S_1, \ldots, S_n\right)$, infinite, $\left\{S_i\right\}_{i=1}^{\infty}$ (we can also consider $\ldots, S_{-1}, S_{0}, S_{1}, \ldots$ written $\left\{S_i\right\}$).

    A collection of random variables $\left(S_1, \ldots S_n\right)$ is specified by the joint probability distribution $p_{S_1, \ldots S_n}$. This is all we need to compute the entropy $H\left(S_1, \ldots, S_n\right)$.
}

\parag{Theorem}{
    Let $S_1, \ldots S_n$ be discrete random variables. Then: 
    \[H\left(S_1, \ldots, S_n\right)\leq H\left(S_1\right) + \ldots+ H\left(S_n\right)\]
    with equality if and only if $S_1, \ldots, S_n$ are independent.
}

\end{document}
