\documentclass[a4paper]{article}

% Expanded on 2022-03-03 at 16:07:07.

\usepackage{../../style}

\title{AICC 2}
\author{Joachim Favre}
\date{Jeudi 01 mars 2022}

\begin{document}
\maketitle

\lecture{3}{2022-03-01}{Kraft paper (wow such humour)}{
\begin{itemize}[left=0pt]
    \item Introduction to source coding and definition of uniquely decodable code and prefix-free code.
    \item Explanation on how to use decoding trees.
    \item Explanation and proof of Kraft-McMillar theorem part 1 and 2.
\end{itemize}

}

\subsection{Source coding}
\parag{Introduction}{
    Our goal is to efficiently describe the source output. For a fixed description alphabet (often binary), we want to minimise the average description length. 
}

\parag{Definition}{
    The \important{encoder} is specified by the input alphabet $\mathcal{A}$, the output alphabet $\mathcal{D}$ (typically $\mathcal{D} = \left\{0, 1\right\}$ since computers love binary), the codebook $\mathcal{C}$ which consists of finite sequences over $\mathcal{D}$, by the one-to-one encoding map $\Gamma : \mathcal{A}^k \mapsto C$, where $k$ is a positive integer (for now we will take $k = 1$).
}

\parag{Example}{
    Let's define the following encoding maps:
    \begin{center}
    \begin{tabular}{c|cccc}
        $\mathcal{A}$ & $\Gamma_O$ & $\Gamma_A$ & $\Gamma_B$ & $\Gamma_C$  \\
        \hline
        $a$ & 00 & 0 & 0 & 0 \\
        $b$ & 01 & 01 & 10 & 01 \\
        $c$ & 10 & 10 & 110 & 011 \\
        $d$ & 11 & 11 & 1110 & 01111
    \end{tabular}
    \end{center}

    We see that the source alphabet $\mathcal{A}$, the $k$, the code alphabet $\mathcal{D}$ and the codebook $\mathcal{C}$ are implicit from this encoding map.

    We can see that, for example $\Gamma_C\left({\color{red}d}a{\color{blue}c}\right) = {\color{red}0111}0{\color{blue}011}$.

    Also, we can see that: 
    \[\Gamma_A\left(ba\right) = 010 = \Gamma_A\left(ac\right)\]
    which might be a problem.
}

\parag{Definition: uniquely decodable}{
    The code is \important{uniquely decodable} if every concatenation of codewords has a unique parsing into a sequence of codewords.

    \subparag{Observation 1}{
        We note that the encoding function $\Gamma$ is one to one by assumption. If we can identify codewords boundaries, we can decode sequence of codewords.
    }
    
    \subparag{Observation 2}{
        A code is uniquely decodable if and only if its reverse (the reverse of 100 is 001) is uniquely decodable.
    }
}

\parag{Example}{
    $\Gamma_B$ from our previous example is uniquely decodable. Indeed, a 0 always marks the end of word.

    $\Gamma_C$ is also uniquely decodable, because 0 marks its beginning.

    $\Gamma_O$ is uniquely decodable as well, since we only read two characters every time. More generally, every fixed-length code is always uniquely decodable.

    However, $\Gamma_A$ is not uniquely decodable since, as we saw, $010 = \Gamma_A\left(ba\right) = \Gamma_A\left(ac\right)$.
}

\parag{Definition: Prefix-free codes}{
    If no codeword is a prefix of another codeword, the code is said to be \important{prefix-free}.

    \subparag{Example}{
        For example, 01 is a prefix of 011, but 10 is not a prefix of 110.

        $\Gamma_O$ and $\Gamma_B$ are prefix free, but $\Gamma_A$ and $\Gamma_C$ are not.

        We see that code $C$ is not prefix-free, yet it is uniquely decodable. 
    }

    \subparag{Observation}{
        We can see that a prefix-free code is uniquely decodable. Since no codeword is the prefix of another, we always know which codeword we are considering (we are not hesitating between 01 or 011 in $\Gamma_C$ for example).
    }

    \subparag{Terminology}{
        A prefix-free code is also called \important{instantaneous code}. Indeed, we can treat data as it arrives. For example, in code $\Gamma_C$, we have to wait more when we receive a 0. This can be a problem in streaming services.
    }
}

\parag{Complete trees}{
    Let's consider all binary sequences of length less than or equal to $\ell_{max}$. We can draw a tree representing it, and then our code in it:
    \imagehere[0.2]{CompleteTreeA.png}

    We can do that for any $D$-ary sequences of length less than or equal to $\ell_{max}$.

    \subparag{Observation}{
        We can see that, for a prefix-free code, then there is no letter before another (there is no letter on the path between the root and any leaf). Recall that code $\Gamma_B$ is prefix free, whereas $\Gamma_C$ is not.
        \imagehere{CompleteTreeBC.png}
    }

}

\parag{Decoding trees}{
    We can remove nodes that are not necessary to make a decoding tree. For example:
    \imagehere[0.35]{DecodingTreeB.png}
}

\parag{Definition: Codeword length}{
    We define the \important{codeword length} to be the size of the codeword (such a beautiful definition).

    We want the average codeword length to be as small as possible.
}

\parag{Theorem: Kraft-McMillan part 1}{
    If a $D$-ary code is uniquely decodable, then its codeword lengths $\ell_1, \ldots, \ell_M$ satisfy: 
    \[D^{-\ell_1} + \ldots + D^{-\ell_M} \leq 1\]

    This is called the Kraft inequality.
    
    \subparag{Contrapositive}{
        If a $D$-ary code's codeword lengths do no satisfy $D^{-\ell_1} + \ldots + D^{-\ell_M} \leq 1$, then this code is not uniquely decodable.
    }
    
    \subparag{Proof for prefix-free}{
        Let a code with codeword lengths $\left\{\ell_1, \ldots, \ell_M\right\}$, and let $L = \max_i \ell_i$. Let us also assume that the code is prefix-free.

        We notice something very important, in a prefix-free code, every terminal leaf belongs to at most one codeword. Thus, for a codeword of length $\ell_i$ consumes $D^{L - \ell_i}$ terminal leaves (all terminal leaves below cannot hold a codeword).

        We know that we have $D^L$ terminal leaves, so we have the following inequality: 
        \[D^{L - \ell_1} + \ldots + D^{L - \ell_M} \leq D^{L} \iff D^{-\ell_1} + \ldots + D^{-\ell_M} \leq 1\]
    }
    
    \subparag{Full proof}{
        Let's take a source alphabet $\mathcal{A} = \left\{a, b, c, d\right\}$. It has 4 elements, but it is similar for any size of alphabets. We will also consider a code alphabet with two elements (binary).

        Let's define $\gamma\left(a\right)$ to be the codeword for $a$ and $\ell\left(a\right)$ to be the size of this codeword, and so on for every letters. Finally, let $\ell_{max} = \max\left\{\ell\left(a\right), \ell\left(b\right), \ell\left(c\right), \ell\left(d\right)\right\}$.

        Let us finally define the following values: 
        \[\phi = \sum_{x \in A}^{} 2^{-\ell\left(x\right)}, \mathspace I_k = \left(\sum_{x \in A}^{} 2^{-\ell\left(x\right)}\right)^k = \phi^k\]
        
        Note that we want to show that $\phi \leq 1$. Showing that $I_k$ grows more slowly than a linear function for all $k$ would be exactly what we need, since then, necessarily, $\phi \leq 1$. Let us begin with $I_2$, we will generalise everything to $I_k$ right after:
        \begin{multiequality}
        I_2 =\ & \left(2^{-\ell\left(a\right)} + 2^{-\ell\left(b\right)} + 2^{-\ell\left(c\right)} + 2^{-\ell\left(d\right)}\right)^2  \\
        =\ & 2^{-\left(\ell\left(a\right) + \ell\left(a\right)\right)} + 2^{-\left(\ell\left(a\right) + \ell\left(b\right)\right)} + 2^{-\left(\ell\left(a\right) + \ell\left(c\right)\right)} + \ldots + 2^{-\left(\ell\left(d\right) + \ell\left(d\right)\right)} 
        \end{multiequality}
        
        This is the sum for some new code $\widetilde{\Gamma}_2$ where we concatenate two elements:
        \begin{center}
        \begin{tabular}{c|c}
            $aa$ & $\left(\gamma\left(a\right), \gamma\left(a\right)\right)$  \\
            $ab$ & $\left(\gamma\left(a\right), \gamma\left(b\right)\right)$  \\
            $\vdots$ & $\vdots$  \\
            $dd$ & $\left(\gamma\left(d\right), \gamma\left(d\right)\right)$  \\
        \end{tabular}
        \end{center}

        However, we know that our initial code is uniquely decodable. Thus, by definition, this new code --- that was obtained through concatenation --- is also uniquely decodable. 

        Let us now reindex our sum $I_2$: 
        \[I_2 = \sum_{m=1}^{2\ell_{max}} N\left(m\right) 2^{-m}\]
        where $N\left(m\right)$ is the number of terms for which $\ell\left(x_1\right) + \ell\left(x_2\right) = m$.

        We know that we can make a total of $2^m$ words of length $m$ in binary. Since our new code is uniquely decodable, each word is present at most once, and thus the number of terms for which $\ell\left(x_1\right) + \ell\left(x_2\right) = m$ is bounded above by $2^m$. In other words, $N\left(m\right) \leq 2^m$. 

        Plugging this result in our sum, we get: 
        \[I_2 = \sum_{m=1}^{2\ell_{max}} 1 = 2\ell_{max}\]
        
        Now, let's generalise what we have done to the $k$\Th power: 
        \[I_k = \left(\sum_{x \in A}^{} 2^{-\ell\left(x\right)}\right)^k = \sum_{x_1 \in A}^{} \sum_{x_2 \in A}^{} \ldots \sum_{x_k \in A}^{} 2^{-\left(\ell\left(x_1\right) + \ldots + \ell\left(x_k\right)\right)} \]

        Similarly, we can also reindex our sum: 
        \[I_k = \sum_{m=1}^{k \ell_{max}} N\left(m\right) 2^{-m} \leq \sum_{m=1}^{k \ell_{max}} 2^{m} 2^{-m} = k \ell_{max}\]
        
        So, we have seen that for all $k \in \mathbb{N}$: 
        \[I_k = \left(\sum_{x \in A}^{} 2^{-\ell\left(x\right)}\right)^{k} \leq k \ell_{max} \iff \phi^k \leq k \ell_{max}\]

        We know that the only possibility to have an exponential less than a linear function for all $k$ is to have $\phi \leq 1$, and thus: 
        \[\sum_{x \in A}^{} 2^{-\ell\left(x\right)} \leq 1\]
        
        \qed
    }
    
}

\parag{Example}{
    For code $\Gamma_O$, we have: 
    \[2^{-2} + 2^{-2} + 2^{-2} + 2^{-2} = 1 \leq 1\]

    For code $\Gamma_A$, we have: 
    \[2^{-1} + 2^{-2} + 2^{-2} + 2^{-2} = 1.25 > 1\]

    So we know that $\Gamma_A$ could not be uniquely decodable. 
}

\parag{Theorem: Kraft-McMillan part 2}{
    If the positive integers $\ell_1, \ldots, \ell_M$ satisfy Kraft's inequality for some positive integer $D$, there exists a $D$-ary prefix-free code (hence uniquely decodable) that has codeword lengths $\ell_1, \ldots, \ell_M$.

    \subparag{Observation}{
        Note that this theorem looks like the converse of the previous one, however it is weaker than it. The converse of the Kraft-McMillan theorem part 1 is wrong.
    }

    \subparag{Example}{
        We will prove this theorem using an example. We can prove it formally, but it requires a lot of notations. 

        Let $\ell_1 = 1$, $\ell_2 = 2$, $\ell_3 = 3$, $\ell_4 = 4$. We notice that it respects the Kraft inequality, so we want to construct a prefix-free code with the given codeword lengths. Let us sort the lengths in increasing order, and draw our tree:
        \imagehere[0.75]{KraftInequalityPart2.png}

        The reason it works, is because every codeword can consume enough terminal leaves.
    }
}

\parag{Summary}{
    We know that if a $D$-ary code is uniquely decodable, then its codeword lengths satisfy Kraft's inequality. We also know that if positive integers satisfy Kraft's inequality, then there exists a $D$-ary prefix-free code that has those codeword lengths.

    Those two imply that any uniquely decodable code can be substituted by a prefix-free code of the same codeword lengths.
}



\end{document}
