\documentclass[a4paper]{article}

% Expanded on 2022-03-08 at 15:30:32.

\usepackage{../../style}

\title{AICC 2}
\author{Joachim Favre}
\date{Mardi 08 mars 2022}

\begin{document}
\maketitle

\lecture{5}{2022-03-08}{Conditional entropy}{
\begin{itemize}[left=0pt]
    \item Explanation on how to use the Huffman algorithm on $D$-ary codes.
    \item Definition of conditional entropy.
    \item Example of pointwise conditional entropy not being less than regular entropy, and proof of complete conditional entropy being bounded above by regular entropy.
    \item Proof of the chain rule for entropy, for two random variables.
\end{itemize}

}

\parag{Ternary Huffman Code}{
    Let's draw the following tree in ternary:
    \svghere[0.5]{HuffmanCodeTernaryFalse.svg}

    However, we have a problem since we cannot connect three at the last stage, and we do not get the most optimal code. We can easily improve it by connecting one of the leaf directly to the root.

    We observe that a ternary tree ``must'' have an odd number of leaves. More generally, in the Huffman strategy, we reduce the alphabet by $D - 1$ symbols every step. Moreover, at the end, we want exactly $D$ symbols, meaning we need $D + k\left(D - 1\right)$ symbols to begin with, where $k \in \mathbb{Z}$. Thus, we can fake it by adding a bunch of symbols having probability 0. Doing this again with our tree:
    \svghere[0.5]{HuffmanCodeTernaryRight.svg}

    We can prove that this is also the optimal strategy.
}

\subsection{Conditional entropy}
\parag{Compressing long string}{
    Let's consider the following source:
    \begin{center}
    \begin{tabular}{c|c|c}
        $\mathcal{A}$ & $p_S\left(s\right)$ & Huffman code \\
        \hline
        $a$ & $2 / 3$ & 0 \\
        $b$ & $1 / 6$ & 10 \\
        $c$ & $1 / 6$ & 11
    \end{tabular}
    \end{center}

    It has entropy $H\left(S\right) = \SI{0.9183}{\bit}$ and average length $L = \frac{4}{3}$.

    Now let's consider sequences of length 2, concatenating two characters. Instead of concatenating two elements of our current Huffman code (getting for example $\Gamma\left(ab\right) = 010$), we can do a Huffman code on those 9 new characters:
    \begin{center}
    \begin{tabular}{c|c|c}
        $\mathcal{A}$ & $p_S\left(s\right)$ & Huffman code \\
        \hline
        $aa$ & $4 / 9$ & 0 \\
        $ab$ & $2 / 18$ & 101 \\
        $\vdots$ & $\vdots$ & $\vdots$ \\
        $cc$ & $1 / 36$ & 1111
    \end{tabular}
    \end{center}

    Doing so, we get average length of $\frac{92}{36}$ instead of $2\cdot \frac{4}{3}$, which is better.
}

\parag{Corollary}{
    We proved that $H_D\left(S\right) \leq L\left(S, \Gamma_{SF}\right) < H_D\left(S\right) + 1$, where $L\left(S, \Gamma_{SF}\right)$ is the average length of a Shannon-Fano code.

    Since we know that Huffman is more optimal than the Shannon-Fano code (since it is the most optimal code) and that it is bounded below by entropy as all codes, we also have this inequality for Huffman codes.
}

\parag{Theorem}{
    The per-letter average codeword length of a $D$-ary Shannon-Fano code for the random variables $\left(S_1, S_2\right)$ fulfills:
    \[\frac{H_D\left(S_1, S_2\right)}{2} \leq \frac{L\left(\left(S_1, S_2\right), \Gamma_{SF}\right)}{2} \leq \frac{H_D\left(S_1, S_2\right) + 1}{2}\]

    In our current example, $S_1$ and $S_2$ are independent and have the same distribution, thus $H_{D}\left(S_1, S_2\right)= 2H_D\left(S\right)$. Thus:
    \[H_D\left(S\right) \leq \frac{L\left(\left(S_1, S_2\right), \Gamma_{SF}\right)}{2} < H_D\left(S\right) + \frac{1}{2}\]

    More generally, with $n$ symbols:
    \[H_D\left(S\right) \leq \frac{L\left(\left(S_1, \ldots, S_n\right), \Gamma_{SF}\right)}{n} < H_D\left(S\right) + \frac{1}{n}\]

    So, on average, the average length per letter goes to entropy as $\frac{1}{n}$. Thus, an optimal strategy to compress a string of independent and identical symbols (iid), is to do a Huffman code over infinitely many symbols (or a very large number on a real computer). There are some problems with that, because of the complexity of finding the Huffman tree, and the difficulty to store a huge tree. We will not consider such problem in this course.

    Instead, we will consider another problem.
}

\parag{Not iid strings}{
    Let's consider strings that are not iid: if $a$ has probability $\frac{2}{3}$, then $aa$ has not necessary probability $\frac{4}{9}$. For example, in French and in English, after a $q$, it is very probable to have a $u$. Also, audio (songs or speeches are good examples), images, or video recording have a lot of repetitions; they are very predictable.
}

\parag{Theorem}{
    The per-letter average codeword-length of a $D$-ary Shannon-Fano code for the random variable $\left(S_1, \ldots, S_n\right)$ fulfills:
    \[\frac{H_D\left(S_1, \ldots, S_n\right)}{n} \leq \frac{L\left(\left(S_1, \ldots, S_n\right), \Gamma_{SF}\right)}{n} < \frac{H_D\left(S_1, \ldots, S_n\right)}{n} + \frac{1}{n}\]

    \subparag{Observation}{
        We want to understand the term $\frac{H_D\left(S_1, \ldots, S_n\right)}{n}$, since the per-letter average codeword length goes to that when $n \to \infty$.

        To do that, we will need to understand conditional property, and how to mode infinitely many random variables (random processes).
    }
}

\parag{Probability recall}{
    Before beginning to develop our theory, let's recall what we had sen on probabilities.

    Let's say we know the following joint distribution:
    \begin{center}
    \begin{tabular}{c|cc||c}
        $p\left(x, y\right)$ & $Y = 0$ & $Y = 1$ & \\
        \hline
        $X = 0$ & 0.4 & 0.2 & 0.6 \\
        $X = 1$ & 0.1 & 0.3 & 0.4 \\
        \hhline{=|==#=}
                         & 0.5 & 0.5
    \end{tabular}
    \end{center}

    We can now compute the following marginal probability:
    \[p_X\left(0\right) = 0.4 + 0.2 = 0.6\]

    Remember that, when it is redondent, we can remove the subscripts:
    \[p_{X, Y}\left(x,y\right) = p\left(x, y\right)\]

    Now, we may want to compute $p\left(x|y\right)$:
    \begin{center}
    \begin{tabular}{c|cc}
        $p\left(x | y\right)$ & $Y = 0$ & $Y = 1$ \\
        \hline
        $X = 0$ & $\frac{0.4}{0.5} = 0.8$ & $\frac{0.2}{0.5} = 0.4$ \\
        $X = 1$ & $\frac{0.1}{0.5} = 0.2$ & $\frac{0.3}{0.5} = 0.6$
    \end{tabular}
    \end{center}

    Or $p\left(y|x\right)$:
    \begin{center}
    \begin{tabular}{c|cc}
        $p\left(y | x\right)$ & $Y = 0$ & $Y = 1$ \\
        \hline
        $X = 0$ & $\frac{0.4}{0.6} = \frac{2}{3}$ & $\frac{0.2}{0.6} = \frac{1}{3}$ \\
        $X = 1$ & $\frac{0.1}{0.4} = \frac{1}{4}$ & $\frac{0.3}{0.4} = \frac{3}{4}$
    \end{tabular}
    \end{center}

}


\parag{Example: Bit-Flipper Channel}{
    Let's say we have a channel $y$ that can take the value 0 or 1, as a uniform distribution. It has a probability $\epsilon$ when equal to 0 to go to 1, and a probability $\delta$ when equal to 1 to go to 0. We can make the following diagram:
    \imagehere[0.3]{BitFlipperDiagram.png}

    We can draw the table of conditional probability:
    \begin{center}
    \begin{tabular}{c|cc}
        $p\left(x|y\right)$ & $y = 0$ & $y = 1$ \\
        \hline
        $x = 0$ & $1 - \epsilon$ & $\delta$ \\
        $x = 1$ & $\epsilon$ & $1 - \delta$
    \end{tabular}
    \end{center}

    Now, assuming that $Y$ is uniformly distributed, we can draw the table of joint distribution (we know $p\left(x, y\right) = p\left(y\right)p\left(x|y\right)$):
    \begin{center}
    \begin{tabular}{c|cc}
        $p\left(x, y\right)$ & $y = 0$ & $y = 1$ \\
        \hline
        $x = 0$ & $\frac{1}{2}\left(1 - \epsilon\right)$ & $\frac{1}{2}\delta$ \\
        $x = 1$ & $\frac{1}{2}\epsilon$ & $\frac{1}{2}\left(1 - \delta\right)$
    \end{tabular}
    \end{center}
}

\parag{Definition}{
    The conditional expectation of $X$ given $Y = y$ is defined as:
    \[\exval\left[X |Y = y\right] \over{=}{def} \sum_{x \in X}^{} xp_{X|Y}\left(x | y\right)\]
}


\parag{Definition: Condition entropy}{
    The conditional entropy of $X$ given $Y = y$ is defined as:
    \[H_D\left(X|Y=y\right) \over{=}{def} \sum_{x \in \mathcal{X}}^{} p_{X|Y}\left(x|y\right) \log_D\left(p_{X|Y}\left(x|y\right)\right)\]

    \subparag{Intuition}{
        $p_{X|Y}\left(\cdot|y\right)$ is a probability distribution on the alphabet of $X$, just like $p_X\left(\cdot\right)$. We know every probability distribution has an entropy associated to it.
    }
}

\parag{Example}{
    Let's get back to the Bit-Flipper Channel. We can compute $H\left(X | Y = 0\right)$:
    \begin{multiequality}
    H\left(X|Y=0\right) =\ & -p_{X|Y}\left(0, 0\right) \log\left(p_{X|Y}\left(0, 0\right)\right) - p_{X|Y}\left(1, 0\right) \log\left(p_{X|Y}\left(1, 0\right)\right) \\
    =\ & -\left(1 - \epsilon\right)\log\left(1 - \epsilon\right) - \epsilon \log\left(\epsilon\right) \\
    =\ & h\left(\epsilon\right)
    \end{multiequality}
    where $h\left(\epsilon\right)$ is the binary entropy function. We can notice that it does not depend on the marginal distribution of $Y$.
}

\parag{Theorem}{
    The conditional entropy of a discrete random variable $X \in \mathcal{X}$conditioned on $Y = y$ satisfies:
    \[0 \leq H_D\left(X | Y = y\right) \leq \log_D\left|\mathcal{X}\right|\]
    with equality on the left if and only if the probability distribution is degenerate (there is 100\% chance of an event happening and 0\% for the others) and equality on the right if and only if the probability distribution is uniform.

    \subparag{Proof}{
       The proof is exactly the same as for the basic entropy bounds.
    }
}

\parag{Example}{
    In the Bit-flipper, we have $\mathcal{X} = \left\{0, 1\right\}$. Hence:
    \[\log_2 \left|\mathcal{X}\right| = \log_2\left(2\right) = 1\]

    And, we indeed know that:
    \[0 \leq h\left(\epsilon\right) \leq 1 = \log_{2}\left|\mathcal{X}\right|, \mathspace \forall \epsilon \in \left[0, 1\right]\]
}

\parag{Observation}{
    We notice that the following inequality that does not hold: $H_D\left(X|Y=y\right) \leq H_D\left(X\right)$. This is very weird, since entropy is a measure of uncertainty, and this inequality should thus hold (if we know more information, then the entropy should be less).

    For example, let's take $\delta = 0$. Then:
    \[p_X\left(0\right) = \frac{1 - \epsilon}{2}\]

    This yields that the entropy of $X$ is given by:
    \[H\left(X\right) = h\left(\frac{1 - \epsilon}{2}\right)\]

    But, we had computed:
    \[H\left(X|Y = 0\right) = h\left(\epsilon\right)\]
    \[H\left(X|Y = 1\right) = h\left(\delta\right) = h\left(0\right) = 0\]

    Thus, if we have $\epsilon = \frac{1}{2}$, for example, then:
    \[\underbrace{H\left(X| Y = 0\right)}_{= 1} > \underbrace{H\left(X\right)}_{= h\left(\frac{1}{4}\right)} > \underbrace{H\left(X |Y=1\right)}_{= 0}\]

    \subparag{Personal note: Intuition}{
        Let's keep the values of our example, i.e $\delta = 0$ and $\epsilon = \frac{1}{2}$.

        We notice that knowing that $Y = 0$ brings us to a uniform random distribution for $X$, and knowing that $Y = 1$ leads to a degenerate random distribution for $X$. However, knowing nothing on $Y$, makes $X$ follow a distribution between uniform and degenerate, which is thus in between our two bounds.
    }

}

\parag{Definition}{
    We define the \important{conditional entropy} of $X$ given $Y$ is defined as:
    \[H_D\left(X|Y\right) \over{=}{def} \sum_{y \in \mathcal{Y}}^{} p_Y\left(y\right) H\left(X|Y=y\right) = \sum_{y \in \mathcal{Y}}^{} p_Y\left(y\right) \left[-\sum_{x \in \mathcal{X}}^{} p_{X|Y}\left(x|y\right) \log_D\left(p_{X|Y}\left(x|y\right)\right)\right]\]
    This definition is really important.

    \subparag{Intuition}{
        We are computing the average conditional entropy of $X$ given $Y = y$, averaged over all values of $y$ under the marginal distribution $p_Y\left(y\right)$.

        In other words, we are basically doing a weighted average.
    }

    \subparag{Equivalent definition}{
        Note that, as usual, we can define this conditional entropy using expected values:
        \[H_D\left(X|Y\right) = \exval\left[\log_D\left(\frac{1}{p_{X|Y}\left(X|Y\right)}\right)\right]\]

        This can be useful to prove theorems.
    }


}

\parag{Example}{
    In the bit-flipper, we have:
    \[H\left(X|Y\right) = p_Y\left(0\right) h\left(\epsilon\right) + p_Y\left(1\right)h\left(\delta\right) = \frac{h\left(\epsilon\right) + h\left(\delta\right)}{2}\]

    This time, we do use the marginal distribution of $Y$.
}

\parag{Theorem}{
    The conditional entropy of a discrete random variable $X \in \mathcal{X}$ conditioned on $Y$ satisfies:
    \[0 \leq H_D\left(X|Y\right) \leq \log_D\left|\mathcal{X}\right|\]
    with equality on the left if and only if for every $y$ there exists an $x$ such that $p_{X|Y}\left(x|y\right) = 1$, and with equality if and only if $p_{X|Y}\left(x|y\right) = \frac{1}{\left|\mathcal{X}\right|}$ for all $x$ and $y$.
}

\parag{Theorem}{
    For any two discrete random variables $X$ and $Y$, we have:
    \[H_D\left(X|Y\right) \leq H_D\left(X\right)\]
    with equality if and only if $X$ and $Y$ are independent random variables.

    \subparag{Intuition}{
        In other words, in average, the uncertainty about $X$ can only become smaller if we know $Y$. \textit{Note bene}, as we have seen, this is not true point-wise. We may have $H_D\left(X | Y=y\right) > H_D\left(X\right)$ for some values of $y$.
    }

    \subparag{Proof}{
        To simplify the notation, let us write:
        \[\sum_{x}^{} \sum_{y}^{} = \sum_{x,y}^{} \]

        Let's first consider the definition of the conditional entropy:
        \begin{multiequality}
        H_D\left(X|Y\right) =\ & \sum_{y}^{} p\left(y\right) \left[-\sum_{x}^{} p\left(x|y\right) \log_D\left(p\left(x|y\right)\right)\right]  \\
        =\ & \sum_{x}^{} \sum_{y}^{} p\left(y\right) p\left(x|y\right) \log_{D}\left(\frac{1}{p\left(x|y\right)}\right) \\
        =\ & \sum_{x,y}^{} p\left(x,y\right) \log_D\left(\frac{1}{p\left(x|y\right)}\right)
        \end{multiequality}

        Also, we know that:
        \[\sum_{y}^{} p\left(y|x\right) = 1, \mathspace \forall x\]

        Now, let's consider the difference of our two sides, we want to show that it is negative:
        \begin{multiequality}
            & H_D\left(X | Y\right) - H_D\left(x\right)  \\
            =\ & \sum_{x,y}^{} p\left(x, y\right) \log\left(\frac{1}{p\left(x|y\right)}\right) + \sum_{x}^{} p\left(x\right)\log\left(p\left(x\right)\right) \\
            =\ & \sum_{x,y}^{} p\left(x, y\right) \log\left(\frac{1}{p\left(x|y\right)}\right) + \sum_{x}^{} \underbrace{\sum_{y}^{} p\left(y|x\right)}_{= 1} p\left(x\right)\log\left(p\left(x\right)\right) \\
        =\ & \sum_{x,y}^{} p\left(x,y\right) \log\left(\frac{p\left(x\right)}{p\left(x|y\right)}\right) \\
        =\ & \sum_{x,y}^{} p\left(x,y\right) \log\left(\frac{p\left(x\right)p\left(y\right)}{p\left(x|y\right)p\left(y\right)}\right)  \\
        \over{\leq}{IT}\ & \sum_{x,y}^{} p\left(x,y\right) \left(\frac{p\left(x\right)p\left(y\right)}{p\left(x,y\right)} - 1\right) \log_D\left(e\right) \\
        =\ & \sum_{x,y}^{} \left(p\left(x\right)p\left(y\right) - p\left(x,y\right)\right)\log\left(e\right)  \\
        =\ & \log_D\left(e\right)\left[\underbrace{\sum_{x}^{} p\left(x\right) \sum_{y}^{} p\left(y\right)}_{=1} - \underbrace{\sum_{x,y}^{} p\left(x,y\right)}_{=1}\right]\\
        =\ & 0
        \end{multiequality}

        The IT equality tells us that the only way that this is equal to 0 is if $\frac{p\left(x\right)}{p\left(x|y\right)} = 1 \iff p\left(x\right) = p\left(x|y\right)$, meaning that $x$ and $y$ are independent.

        Note that this theorem can also be proven using the expected value definition of conditional entropy, and the linearity of expected values.

        \qed
    }
}

\parag{Example 1}{
    Let $X \in \left\{0, 1\right\}$ be uniformly distributed, and let $Y = X$. Then:
    \[H\left(X|Y\right) = 0 \mathspace \text{and} \mathspace H\left(X\right) = 1\]

    Indeed:
    \[H\left(X|Y\right) = \sum_{y}^{} p\left(y\right)H\left(X|Y=y\right)= \sum_{y}^{} p\left(y\right)\cdot 0 = 0\]
}

\parag{Example 2}{
    Let's get back to our Lisa rolling dice example. She rolls two dice and announces the sum $L$ written as a two digit number. The alphabet of $L = L_1L_2$ is $\left\{02, 03, \ldots, 12\right\}$, the one of $L_1$ is $\left\{0,1\right\}$ and the one of $L_2$ is $\left\{0, 1, \ldots, 9\right\}$.

    We can to determine the probability that $L_2 = 2$, knowing that $L_1 = 1$:
    \[p_{L_2|L_1}\left(2|1\right) = \frac{p_{L_1, L_2}\left(1, 2\right)}{p_{L_1}\left(1\right)} = \frac{\frac{1}{36}}{\frac{1}{6}} = \frac{1}{6}\]

    We can do that for every possibilities. We then get:
    \[H\left(L_2|L_1 = 1\right) = \SI{1.456}{\bit}, \mathspace H\left(L_2|L_1=0\right) = \SI{2.857}{\bit}\]

    So, we can compute:
    \[H\left(L_2|L_1\right) = p_{L_1}\left(0\right) H\left(L_2|L_1 = 0\right) + p_{L_1}\left(1\right) H\left(L_2|L_1 = 1\right) = \SI{2.624}{\bit}\]

    We computed that $H\left(L_2\right) = \SI{3.22}{\bit}$. We can indeed observe that, indeed:
    \[H\left(L_2|L_1\right) \leq H\left(L_2\right)\]
}

\parag{Theorem: Chain rule for entropy}{
    Let $X$ and $Y$ be two random variables. Then:
    \[H_D\left(X, Y\right) = H_D\left(X\right) + H_D\left(Y|X\right)\]

    We already knew it if they were independent, but this result is completely general.

    \subparag{Proof}{
        Let's use the expected value definition of the conditional entropy, and the linearity of the expected value:
        \begin{multiequality}
        H\left(X, Y\right) =\ & \exval\left[-\log\left(p\left(X, Y\right)\right)\right]  \\
        =\ & \exval\left[-\log\left(p\left(X\right) p\left(Y|X\right)\right)\right]  \\
        =\ & \exval\left[-\log\left(p\left(X\right)\right)\right] + \exval\left[-\log\left(p\left(Y|X\right)\right)\right]  \\
        =\ & H\left(X\right) + H\left(Y|X\right)
        \end{multiequality}

        \qed
    }

    \subparag{Observation}{
        We can see that:
        \[H\left(X | Y\right) = H\left(X, Y\right) - H\left(Y\right)\]

        This can be convenient for computations.
    }
}



\end{document}
