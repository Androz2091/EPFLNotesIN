\documentclass[a4paper]{article}

% Expanded on 2022-03-15 at 15:17:21.

\usepackage{../../style}

\title{AICC 2}
\author{Joachim Favre}
\date{Mardi 15 mars 2022}

\begin{document}
\maketitle

\lecture{7}{2022-03-15}{Almost to the fundamental theorem}{
\begin{itemize}[left=0pt]
    \item Definition of the entropy of a symbol, the entropy rate of a source, and regular sources.
    \item Definition of entropy sources.
    \item Proof that a stationary source is regular.
    \item Proof that, for a stationary source, the limit of $\frac{H_D\left(S_1, \ldots, S_n\right)}{n}$ as $n$ tends towards infinity, is equal to the entropy of a symbol.
    \item Definition of different sources, such as the Sunny-Rainy source, the Green-Blue source and the Weekly-coin-flip source, and applying them to some examples.
\end{itemize}
}

\parag{Proposition}{
    Let $X, Y, Z$ be three random variables. Then:
    \[p\left(X, Y, Z\right) = p\left(X\right)p\left(Y|X\right)p\left(Z|X, Y\right)\]

    \subparag{Proof}{
        Thanks to the definition of conditional probability, we know that: 
        \[p\left(X, Y\right) = p\left(X\right) p\left(Y | X\right)\]

        Now, let's define a new random variable $W = \left(X, Y\right)$. Thus: 
        \begin{multiequality}
        p\left(X, Y, Z\right) =\ & p\left(W, Z\right) = p\left(W\right)p\left(Z|W\right)  \\
        =\ & p\left(X, Y\right)p\left(Z|X,Y\right) = p\left(X\right)p\left(Y|X\right)p\left(Z|X,Y\right) 
        \end{multiequality}

        \qed
    }

    \subparag{Remark}{
        Note that this can be easily generalised to $n$ random variables.
    }
    
}

\parag{Sunny-rainy source}{
    Let's say that we know that the weather, which can be sunny or rainy, only depends on the day right before. The weather has probability $q = \frac{6}{7}$ of not changing, and $1 - q = \frac{1}{7}$ of switching. The weather of the first day is chosen uniformly.

    We get that: 
    \begin{functionbypart}{p_{S_i | S_1, \ldots S_{i-1}} \left(s_i | s_1, \ldots, s_{i-1}\right) = p_{S_i |S_{i-1}} \left(s_i |s_{i-1}\right)}
    q, \mathspace s_i = s_{i-1} \\
    1 - q, \mathspace s_i \neq s_{i-1}
    \end{functionbypart}

    Let us write 0 for rain, and 1 for sun. We have: 
    \[p_{S_1}\left(\text{sun}\right) = p_{S_1}\left(1\right) = \frac{1}{2}\]
    \[p_{S_1, S_2}\left(1, 0\right) = p_{S_1}\left(1\right) p_{S_2 | S_1}\left(0 |1\right) = \frac{1}{2} \cdot \left(1 - q\right) \]

    We can also marginalise:
    \[p_{S_2}\left(1\right) = \sum_{x}^{} p_{S_1, S_2}\left(x, 1\right) = p_{_1, S_2}\left(0, 1\right) + p_{S_1, S_2}\left(1, 1\right) = \frac{1}{2}\left(1 - q\right) + \frac{1}{2}q = \frac{1}{2}\]

    Very generally, we can find: 
    \[p_{S_i}\left(s_i\right) = \frac{1}{2}\]

    Let's compute a much trickier one: 
    \begin{multiequality}
    p_{S_3, S_1}\left(1 | 0\right) =\ & \frac{p_{S_1, S_3}\left(0, 1\right)}{p_{S_1}\left(0\right)}  \\
    =\ & \frac{\sum_{x}^{}  p_{S_1, S_2, S_3}\left(0, x, 1\right)}{p_{S_1}\left(0\right)}  \\
    =\ & \frac{\sum_{x}^{} p_{S_1}\left(0\right) p_{S_2 | S_1}\left(x | 0\right) p_{S_3|S_1, S_2}\left(1 | 0, x\right)}{p_{S_1\left(0\right)}} \\
    =\ &  \sum_{x}^{} p_{S_2 |S_1}\left(x | 0\right) p_{S_3 | S_2}\left(1 |x\right)  \\
    =\ & p_{S_2 | S_1}\left(0 | 0\right) p_{S_3 | S_2}\left(1 | 0\right) + p_{S_2 | S_1}\left(1 | 0\right) p_{S_3 |S_2}\left(1 | 1\right)  \\
    =\ & q\left(1 - q\right) + q\left(1 - q\right)  \\
    =\ & 2q\left(1 - q\right) 
    \end{multiequality}
    
    We can understand this result by drawing the following diagram. There are two possible paths to reach $S_3 = 1$ from $S_1 = 0$, which both have probability $q\left(1 - q\right)$, and are independent:
    \svghere{SunnyRainSourceDiagram.svg}

    Thus, $S_i$ and $S_{i-1}$ are not independent. Similarly, $S_i$ and $S_{i-2}$ are neither.
    
    Note that if we set $q = \frac{1}{2}$, we get back to the coin-flipping source. Indeed, then, the weather at the day before no longer matters (since we have probability $\frac{1}{2}$ in both cases, and thus $p_{S_i | S_{i-1}} = p_{S_i}$).
}

\parag{Definition}{
    We define the \important{entropy of a symbol} as:
    \[H\left(\mathcal{S}\right) = \lim_{n \to \infty} H\left(S_n\right)\]

    We also define the \important{entropy rate of the source} $\mathcal{S}$:
    \[H^*\left(\mathcal{S}\right) = \lim_{n \to \infty} H\left(S_n | S_1, \ldots, S_{n-1}\right)\]

    The source is said to be \important{regular} if both exist and are finite.

    \subparag{Remark}{
        Note that, in French entropy rate is translated ``entropie par symbole'', meaning ``entropy per symbol''. This may give more intuition on what this means.
    }
    
}

\parag{Coin flip source entropy}{
    For the coin flip source, we found: 
    \[p_{S_i}\left(s_i\right) = \frac{1}{2} \implies H\left(S_i\right) = \SI{1}{\bit}, \mathspace \forall i\]
    \[p_{S_i | S_1, \ldots, S_{i-1}}\left(s_i | s_1, \ldots, s_{i-1}\right) = \frac{1}{2} \implies H\left(S_i | S_{1}, \ldots, S_{i-1}\right) = \SI{1}{\bit}, \mathspace \forall i\]

    Thus, we know that: 
    \[H\left(\mathcal{S}\right) = \lim_{n \to \infty} H\left(S_n\right) = \SI{1}{\bit}\]
    \[H^*\left(\mathcal{S}\right) = \lim_{n \to \infty} H\left(S_n | S_1, \ldots, S_{n-1}\right) = \SI{1}{\bit}\]

    Which means that this source is regular.
}

\parag{Sunny-rainy source entropy}{
    In the sunny-rainy source, we know that: 
    \[p_{S_i}\left(s_i\right) = \frac{1}{2} \implies H\left(S_i\right) = \SI{1}{\bit}, \mathspace \forall i\]

    To compute $H\left(S_i | S_1, \ldots, S_{i-1}\right)$, we can remember that 
    \begin{multiequation}
    & p\left(s_i | s_1, \ldots, s_{i-1}\right) = p\left(s_i | s_{i-1}\right) \\
    \implies & H\left(S_i | S_1 = s_1, \ldots, S_{i-1} = s_{i-1}\right) = H\left(S_i | S_{i-1}=s_{i-1}\right)
    \end{multiequation}
    
    However, we can also see that: 
    \[H\left(S_i | S_{i-1} = 0\right) = h\left(q\right) = H\left(S_i | S_{i-1} = 1\right)\]
    
    So we can now compute our conditional entropy:
    \[H\left(S_i | S_{i-1}\right) = p\left(S_{i-1} = 1\right) h\left(q\right) + p\left(S_{i-1} = 0\right)h\left(q\right) = h\left(q\right)\]
    
    Plugging in $q = \frac{6}{7}$, we have: 
    \[H\left(S_i | S_{i-1}\right) \approx \SI{0.592}{\bit}, \mathspace \forall i\]

    We can finally compute the entropy of a symbol and the entropy rate of the source:
    \[H\left(\mathcal{S}\right) = \lim_{n \to \infty} H\left(S_n\right) = \SI{1}{\bit}\]
    \[H^*\left(\mathcal{S}\right) = \lim_{n \to \infty} H\left(S_n | S_1, \ldots, S_{n-1}\right) = \SI{0.592}{\bit}\]

    Which means that this source is also regular.
}

\parag{Green-blue source}{
    This source models a sequence $S_1, \ldots, S_n$ of a person's votes from the alphabet $\mathcal{A} = \left\{G, B\right\}$. The first vote is chosen uniformly in $\mathcal{A}$, and the next votes are identical to the initial vote.

    We have that: 
    \[p_{S_n}\left(s_n\right) = \frac{1}{2}\]
    
    Indeed, we do not know anything, and the $n$\Th vote is defined by the first vote, with probability $\frac{1}{2}$.

    The entropy of a symbol is 1: 
    \[H\left(\mathcal{S}\right) = \lim_{n \to \infty} H\left(S_n\right) = \lim_{n \to \infty} 1 = 1\]
    

    The entropy rate is clearly 0, since $p_{S_n | S_{n-1}}$ is a degenerate random distribution:
    \[H^*\left(\mathcal{S}\right) = \lim_{n \to \infty} H\left(S_n | S_{n-1}\right) = \lim_{n \to \infty} 0 = 0\]
    
    We deduce that the source is regular. Note that this is the sunny-rainy source with $q = 0$.
}

\parag{Weekly-coin-flip source}{
    The source models a sequence $S_1, S_2, \ldots$ of coin flips $\mathcal{A} = \left\{H, T\right\}$ such that: 
    \[p_{S_{i + 7k}}\left(T\right) = \frac{1}{i}, \mathspace i \in \left\{1, \ldots, 7\right\}, k \in \mathbb{N}\]
    
    Thus, for example: 
    \[p_{S_1}\left(T\right) = 1 = p_{S_8}\left(T\right)\]
    \[p_{S_2}\left(T\right) = \frac{1}{2}\]

    This is not regular, since the symbol entropy does not converge. On Mondays, the symbol entropy is $\SI{0}{\bit}$, on Tuesday it is $\SI{1}{\bit}$ and so on. It clearly oscillates.
}

\parag{Stationary sources}{
    Let us say that $\left(S_1, \ldots, S_n\right)$ are distributed according to $p_{S_1, \ldots, S_n}\left(x_1, \ldots, x_n\right)$, which we can write as: 
    \[\left(S_1, \ldots, S_n\right) \sim p_{S_1, \ldots, S_n}\left(x_1, \ldots, x_n\right)\]
    
    We can also see have a shifted version: 
    \[\left(S_{k+1}, \ldots, S_{k+n}\right) \sim p_{S_{k+1}, \ldots, S_{k+n}}\left(x_1, \ldots, x_n\right)\]
    
    We call the source to be \important{stationary} if for all $n$ and $k$: 
    \[p_{S_1, \ldots, S_n}\left(x_1, \ldots, x_n\right) = p_{S_{k+1}, \ldots, S_{k+n}}\left(x_1, \ldots, x_n\right), \mathspace \forall x_1, \ldots, x_n\]
    
    \subparag{Intuition}{
        Basically, the idea is that if we look at a local information of a stationary source, we have no information on where we are.
    }
    
    \subparag{Implication}{
        A stationary source has the following properties: 
        \[p_{S_1} = p_{S_m}, \mathspace \forall m\]
        \[p_{S_1, S_2} = p_{S_m, S_{m+1}}, \mathspace \forall m\]
        \[p_{S_1, S_2, S_3} = p_{S_m, S_{m+1}, S_{m+2}}, \mathspace \forall m\]

        A more complicated property is:
        \[p_{S_m, S_n} = p_{S_{m+k}, S_{n+k}}\]

        This last property can be proven through marginalisation.

        Thus, for any subset $\mathcal{I}$ of indices (even indices that do not follow each other ($\mathcal{I} = \left\{0, 3, 7\right\}$ for instance)): 
        \[p_{S_\mathcal{I}} = p_{S_{\mathcal{I} + k}}\]
        
    }
}

\parag{Example}{
    Let's consider our coin-flip example.

    We know that: 
    \[p_{S_1, \ldots, S_n} = \prod_{i=1}^{n} p_{S}\left(s_i\right) = \frac{1}{2^n}\]
    
    Also: 
    \[p_{S_{k+1}, \ldots, S_{k+n}} = \prod_{i=1}^{n} p_{S}\left(s_{i+k}\right) = \frac{1}{2^n}\]

    Thus, since both are equal, the coin-flip source is stationary. We can do a similar reasoning to prove that the sunny-rainy source and the green-blue source are stationary.  However, the weekly-coin-flip is not stationary, since $p_{S_1}\left(s\right) \neq p_{S_2}\left(s\right)$.
}

\parag{Theorem: Conditioning reduces entropy 2}{
    For any three discrete random variables $X, Y$ and $Z$: 
    \[H_D\left(X|Y, Z\right) \leq H_D\left(X | Z\right)\]
    with equality if and only if $X$ and $Y$ are conditionally independent random variables given $Z$ (meaning $p\left(x, y|z\right) = p\left(x|z\right)p\left(y|z\right)$ for all $x, y, z$).

    \subparag{Proof}{
        The proof is similar to the first version of this theorem.
    }

    \subparag{Remark}{
        Note that this can be easily generalised to $n$ random variables.
    }
}

\parag{Theorem}{
    A stationary source is regular.

    \subparag{Proof}{
        We know that $H\left(S_i\right)$ are equal for all $i$, thus: 
        \[\lim_{n \to \infty} H\left(S_n\right) \text{ exists}\]
        
        Let's now consider the entropy rate of the source. We know that $H\left(S_2 | S_1\right) \leq H\left(S_2\right)$ since conditioning reduce entropy. Moreover, since the source is stationary, we have that $H\left(S_2\right) = H\left(S_1\right)$ and thus: 
        \[H\left(S_2 | S_1\right) \leq H\left(S_1\right)\]

        Similarly, we know that $H\left(S_3 | S_1, S_2\right) \leq H\left(S_3 | S_2\right)$ since conditioning reduces entropy, and we know that $H\left(S_3 | S_2\right) = H\left(S_2 | S_1\right)$ since the source is stationary, thus: 
        \[H\left(S_3 | S_1, S_2\right) \leq H\left(S_2 | S_1\right)\]
        
        We can continue this way to see that: 
        \[H\left(S_1\right) \geq H\left(S_{2} | S_1\right) \geq \ldots \geq H\left(S_n | S_1, \ldots, S_{n-1}\right) \geq 0\]
        
        However, since entropy is nonnegative, we get that $H\left(S_n | S_1, \ldots, S_{n-1}\right)$ is bounded by $H\left(S_1\right)$ and 0 for all $n$, and thus, since this is a bounded decreasing sequence, we get that $\lim_{n \to \infty} H\left(S_n | S_1, \ldots, S_{n-1}\right)$ exists.

        \qed
    }

    \subparag{Converse}{
        Note that the converse is not true: a regular source is not necessarily stationary.
    }
}

\parag{Theorem}{
    For a stationary source, we have:
    \[H^*\left(\mathcal{S}\right) \leq H\left(\mathcal{S}\right)\]
    with equality if and only if the symbols are independent.

    \subparag{Proof}{
        Since conditioning reduces entropy, we know that: 
        \[H\left(S_n |S_1, \ldots, S_{n-1}\right) \leq H\left(S_n\right)\]

        Since limits preserve inequalities, we get: 
        \[\lim_{n \to \infty} H\left(S_n |S_1, \ldots, S_{n-1}\right) \leq \lim_{n \to \infty}  H\left(S_n\right) \implies H^*\left(\mathcal{S}\right) \leq H\left(\mathcal{S}\right)\]

        Note that there are equalities everywhere when symbols are independent.

        \qed
    }
}

\parag{Theorem: Cesàro means}{
    Let $a_1, a_2, \ldots$ be a real-valued sequence and let $c_1, c_2, \ldots$ be the sequence of running average defined by:
    \[c_n = \frac{a_1 + \ldots + a_n}{n}\]
    
    If $\lim_{n \to \infty} a_n$ exists, then $c_n$ converges as well, and: 
    \[\lim_{n \to \infty} c_n = \lim_{n \to \infty} a_n\]
    
    \subparag{Intuition}{
        When we average stuff, the first terms do not matter, only the ones when we tend towards infinity.
    }
}

\parag{Theorem}{
    For a stationary source, we have: 
    \[\lim_{n \to \infty} \frac{H_D\left(S_1, \ldots, S_n\right)}{n} = H^*\left(\mathcal{S}\right)\]
    
    Moreover, $a_n = \frac{H_D\left(S_1, \ldots, S_n\right)}{n}$ is a non-increasing sequence.

    \subparag{Proof}{
        By the chain rule of entropy, we know that: 
        \[\frac{H_D\left(S_1, \ldots, S_n\right)}{n} = \frac{H_D\left(S_1\right) + H_D\left(S_2 | S_1\right) + \ldots + H_D\left(S_n | S_1, \ldots, S_{n-1}\right)}{n}\]
        
        By the Cesàro means theorem, both sides converge to the limit of $H_D\left(S_n | S_1, \ldots, S_{n-1}\right)$, which is: 
        \[\lim_{n \to \infty} H_D\left(S_n | S_1, \ldots, S_{n-1}\right) = H^*\left(\mathcal{S}\right)\]

        We still have to prove that $\frac{H_D\left(S_1, \ldots, S_n\right)}{n}$ is non-increasing, i.e. that: 
        \[\frac{H_D\left(S_1, \ldots, S_n\right)}{n} \geq \frac{H_D\left(S_1, \ldots, S_{n+1}\right)}{n+1}\]

        Which is equivalent to: 
        \begin{multiequality}
        \left(n+1\right)H_D\left(S_1, \ldots, S_n\right) \geq\ & nH_D\left(S_1, \ldots, S_{n+1}\right)  \\
        =\ & n\left(H\left(S_{n+1} | S_1, \ldots, S_n\right) + H\left(S_1, \ldots, + S_n\right)\right) 
        \end{multiequality}
        
        We notice that if we prove that $H_D\left(S_1, \ldots, S_n\right) \geq nH_D\left(S_{n+1} | S_1, \ldots, S_n\right)$, we will have won:
        \begin{multiequality}
        & H_D\left(S_1, \ldots, S_n\right)  \\
        =\ & H_D\left(S_1\right) + H_D\left(S_2 | S_1\right) + \ldots + H_D\left(S_n, \ldots, S_{n-1}\right) \\
        \over{=}{$\dagger$} \ & H_D\left(S_{n+1}\right) + H_D\left(S_{n+1}| S_n\right) + \ldots + H_D\left(S_{n+1} | S_2, \ldots, S_n\right) \\
        \over{\geq}{*} \ &  H_D\left(S_{n+1} | S_1, \ldots, S_n\right) + \ldots + H_D\left(S_{n+1} |S_1, \ldots, S_n\right) \\
        =\ & nH_D\left(S_{n+1} | S_1, \ldots, S_n\right) 
        \end{multiequality}

        Note that the equality $\dagger$ holds since the source is stationary, and the inequality * holds because conditioning reduces entropy.
        
        \qed
    }
}

\parag{Example: Coin-flip source}{
    We want to compute $H\left(S_1, \ldots, S_n\right)$ for the coin-flip source.

    Since the source produces independent and identically distributed (iid) symbols, we get: 
    \[H\left(S_1, \ldots, S_n\right) = H\left(S_1\right) + \ldots + H\left(S_n\right) = nH\left(S_1\right)\]

    The first identity holds because the symbols are independent, and the second one because they are identically distributed.

    Moreover, since the distribution is uniform, we know that $H\left(S_1\right) = \SI{1}{\bit}$, thus: 
    \[H\left(S_1, \ldots, S_n\right) = \SI{n}{\bit}\]

    Which leads to:
    \[H^*\left(\mathcal{S}\right) = \lim_{n \to \infty} \frac{H\left(S_1, \ldots, S_n\right)}{n} = \lim_{n \to \infty} \SI{1}{\bit} = \SI{1}{\bit}\]
    as expected.
}

\parag{Example: Sunny-rain source}{
    We want to compute $H\left(S_1, \ldots, S_n\right)$ for the sunny-rain source with $q = \frac{6}{7}$.

    By the chain rule, we know that: 
    \[H\left(S_1, \ldots, S_n\right) = H\left(S_1\right) + H\left(S_2 | S_1\right) + \ldots + H\left(S_n | S_1, \ldots, S_{n-1}\right)\]

    However, since only the day before matters: 
    \[H\left(S_1, \ldots, S_n\right) = H\left(S_1\right) + H\left(S_2 | S_1\right) + \ldots + H\left(S_n | S_{n-1}\right) = \SI{1}{\bit} + 0.592\left(n-1\right)~\si{\bit}\]

    Which leads to: 
    \[H^*\left(\mathcal{S}\right) = \lim_{n \to \infty} \frac{H\left(S_1, \ldots, S_n\right)}{n} = \lim_{n \to \infty} \frac{1 + 0.592\left(n-1\right)}{n}~\si{\bit} = \SI{0.592}{\bit}\]
    as expected.
}

\end{document}
