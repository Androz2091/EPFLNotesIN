\documentclass[a4paper]{article}

% Expanded on 2022-05-15 at 16:30:06.

\usepackage{../../style}

\title{AICC 2}
\author{Joachim Favre}
\date{Jeudi 12 mai 2022}

\begin{document}
\maketitle

\lecture{22}{2022-05-12}{Matrices!}{
\begin{itemize}[left=0pt]
    \item Definition of subspaces.
    \item Definition of linear combination, span, linear independence, basis, canonical basis, and dimension.
    \item Examples of the link between the two ways to describe a subspace: a list of vectors spanning our subspace, or a system of linear homogeneous equations.
\end{itemize}
}

\parag{Example}{
    Let $\mathbb{F}$ be a finite field. Then, $V = \left(\mathbb{F}^n, +, \times\right)$ is a vector space.
    
    For instance, let us pick $\mathbb{F}_2 = \left(\mathbb{Z} / 2\mathbb{Z}, +, \times\right)$ and $n = 6$. Then, we have: 
    \[V = \left\{\left(0, 0, 0, 0, 0, 0\right), \left(0, 0, 0, 0, 0, 1\right), \ldots, \left(1, 1, 1, 1, 1, 1\right)\right\}\]
    
    We can see that this indeed fulfils all the required axioms. We can construct a code out of a subset of $V$.
}

\parag{Definition: Subspace}{
    Let $V$ be a vector space. 

    A set $S \subseteq V$ is called a \important{subspace} if it is non-empty and closed under the vector space operations, meaning that:  
    \[\forall \alpha \in \mathbb{F}\ \left(\bvec{s} \in S \implies \alpha \bvec{s} \in S\right)\]
    \[\bvec{s_1}, \bvec{s_2} \in S \implies \bvec{s_1} + \bvec{s_2} \in S\]

    \subparag{Observation}{
        In particular, we see that $\bvec{0} \in S$ is necessary. Indeed, we have that $0 \in \mathbb{F}$ and we can prove that $0 \cdot \bvec{s} = \bvec{0}$. Thus, by the closure of the multiplication, we need that, for any $\bvec{s} \in S$: 
        \[0 \bvec{s} = \bvec{0} \in S\]
        
        In other words, $\bvec{0} \in S$ is a necessary condition.
    }
    
    \subparag{Remark}{
        A subspace is a vector space: all properties except for closure are inherited from the fact that it is a subset of a vector space.
    }
}

\parag{Example 1}{
    Let us pick $V = \mathbb{R}^2$.

    Every line going through the origin (since we need $\bvec{0} \in S$) represents a subspace. For instance, let us consider the following subspace:
    \[S = \left(\bvec{s} \in V \text{ such that } \bvec{s} = a\left(-2, 1\right) \text{ for some } a \in \mathbb{R}\right)\]

    This gives us the following diagram, where $\bvec{v} = \left(-2, 1\right)$:
    \imagehere[0.3]{ExampleSubspaceR2.png}

    We can also describe this line using the normal. For instance, $\bvec{n} = \left(1, 2\right)$ is orthogonal to our line, thus: 
    \[S = \left\{\bvec{s} = \left(s_1, s_2\right) \in V \text{ such that } \bvec{s} \dotprod \bvec{n} = s_1 + 2s_2 = 0\right\}\]

    Let us now verify closure. Let $\bvec{u}, \bvec{v} \in S$, giving $\bvec{u} + \bvec{v} = \left(u_1 + v_1, u_2 + v_2\right)$. We need to verify if it matches the equation $s_1 + 2s_2 = 0$: 
    \[\left(u_1 + v_1\right) + 2\left(u_2 + 2v_2\right) = \underbrace{\left(u_1 + 2u_2\right)}_{= 0} + \underbrace{\left(v_1 + 2v_2\right)}_{= 0} = 0\]
    since $\bvec{u}, \bvec{v} \in S$.

    We can also verify that this subspace is closed under multiplication.
}

\parag{Example 2}{
    Let's consider $V = \mathbb{F}_7^3$. Thus, we have: 
    \[\bvec{s} = \left(s_1, s_2, s_3\right), \mathspace \text{where } s_i \in \mathbb{F}_7\]
    
    Let us consider the following subset: 
    \[S = \left\{\bvec{s} \in \mathbb{F}_7^3 : s_1 + 2s_2 + s_3 = 0\right\}\]
    
    By a similar argument as in the previous example, this is a subspace. Note that we do not care of the coefficient behind $s_1, s_2$ and $s_3$, when we do not have a constant and set it equal to 0.
}

\parag{Definition: Linear combination}{
    A \important{linear combination} of a list of vectors $\left\{\bvec{v_1}, \ldots, \bvec{v_r}\right\}$ is any vector of the form: 
    \[\sum_{i=1}^{r} \lambda_i \bvec{v}_i, \mathspace \lambda_i \in \mathbb{F}\]
    
}

\parag{Definition: Span}{
    The \important{span} $S$ of a list of vectors is the set of all linear combinations we can make from the list. We say that the list spans $S$.
}

\parag{Definition: Finite-dimensional vector space}{
    A vector space $V$ that is the span of a list of vectors is a \important{finite-dimensional vector space}.

    \subparag{Example}{
        For instance, we have: 
        \[\mathbb{R}^2 = \Span\left\{\begin{pmatrix} 1 \\ 0 \end{pmatrix}, \begin{pmatrix} 0 \\ 1 \end{pmatrix} \right\} = \Span\left\{\begin{pmatrix} 1 \\ 0 \end{pmatrix}, \begin{pmatrix} 1 \\ 1 \end{pmatrix} \right\}\]
    }
}

\parag{Definition: Linear independence}{
    The vectors $\left\{\bvec{v_1}, \ldots, \bvec{v_n}\right\}$ are said to be \important{linearly independent}, if: 
    \[\sum_{i=1}^{n} \lambda_i \bvec{v_i} = \bvec{0} \implies \lambda_i = 0 \text{ for all } i\]
    
    In other words, no vector can be written as a linear combination of the others.

    \subparag{Intuition}{
        Every vector brings something to the list, we cannot drop any of them.
    }
}

\parag{Definition: Basis}{
    A \important{basis} for a finite-dimensional vector space is a list of linearly independent vectors that span $V$.
}

\parag{Theorem}{
    A list $\left\{\bvec{v_1}, \ldots, \bvec{v_n}\right\}$ is a basis \textit{if and only if} every $\bvec{v} \in V$ can be written \textit{uniquely} as a linear combination of this list $\sum_{i=1}^{n} \lambda_i \bvec{v_i}$.

    \subparag{Proof $\implies$}{
        We already know any vector of $V$ can be written as a linear combination (since the list spans $V$ by definition of a basis), we only need to prove unicity.

        Let's say we have: 
        \begin{multiequation}
        & \bvec{v} = \sum_{i = 1}^{n} \lambda_i \bvec{v_i} = \sum_{i=1}^{n} \widetilde{\lambda}_i \bvec{v}_i  \\
        \implies & \sum_{i = 1}^{n} \lambda_i \bvec{v_i} - \sum_{i = 1}^{n} \widetilde{\lambda}_i \bvec{v}_i = \bvec{0}  \\
        \implies & \sum_{i = 1}^{n} \left(\lambda_i -  \widetilde{\lambda}_i\right) \bvec{v}_i = \bvec{0} 
        \end{multiequation}
        
        But since $\left\{\bvec{v_1}, \ldots, \bvec{v_n}\right\}$ is a basis, we know that it implies those vectors are linearly independent, and thus, for all $i$: 
        \[\lambda_i - \widetilde{\lambda}_i = 0 \implies \lambda_i = \widetilde{\lambda}_i\]
        
        Meaning that the linear combination was unique.
    }
}

\parag{Theorem}{
    Every spanning list in a vector space can be reduced to a basis of the vector space.

    \subparag{Algorithm}{
        \begin{enumerate}[left=0pt]
            \item Remove all zero-elements of the list.
            \item Of the new list, remove the second element if it is in the linear span of the first. We repeat this step until we have a list where the second element is not in the linear span of the first.
            \item We do similarly with the third element: we remove it if it is in the linear span of the first two.
            \item We continue until we have considered the last vector of the list. This yields a list of vector that spans the vector space and are linearly independent (else one vector could be written as the linear combination of vectors with smaller index).
        \end{enumerate}
    }
}

\parag{Theorem}{
    Any two bases of a finite-dimensional vector space have the same length.
}

\parag{Definition: Dimension}{
    The \important{dimension} of a finite-dimensional vector space $V$, denoted by $\dim\left(V\right)$, is defined to be the length of any basis of $V$.

}

\parag{Dimension: Properties}{
    Let $V$ be a vector space with $\dim\left(V\right) = n$. Then, we have the following properties.

    \begin{itemize}
        \item A list of $n$ linearly independent vectors in $V$, is a basis of $V$.
        \item A list of $n$ vectors in $V$ spanning $V$, is a basis of $V$.
        \item Any list of $m > n$ vectors in $V$ cannot be linearly independent.
        \item Any list of $m < n$ vectors cannot span $V$.
    \end{itemize}
    
    \imagehere{BasisNVectors.png}
}

\parag{Canonical basis}{
    Let $\mathbb{F}$ be a finite field with $0$ being its additive neutral element and $1$ being its multiplicative element. Then, the \important{canonical basis} of $\mathbb{F}^n$, is: 
    \[\left(\left(1, 0, 0, \ldots, 0, 0\right), \left(0, 1, 0, \ldots, 0, 0\right), \ldots, \left(0, 0, 0, \ldots, 0, 1\right)\right)\]
}

\subsection{Vector spaces over finite fields}
\parag{Lemma}{
    Let $\mathbb{F}$ be a finite field with cardinality $\left|\mathbb{F}\right|$. Then, the vector space $V = \mathbb{F}^n$ is finite and has cardinality: 
    \[\left|V\right| = \left|\mathbb{F}\right|^n\]

    \subparag{Implication}{
        This implies that $\mathbb{F}_p^n$ has cardinality $p^n$.

        For instance, the following has $2^n$ vectors: 
        \[F_2^n = \left\{\left(0, \ldots, 0, 1\right), \left(0, \ldots, 1, 0\right), \left(0, \ldots, 1, 1\right), \ldots, \left(1, \ldots, 1, 1\right)\right\}\]
    }
}

\parag{Theorem}{
    A $k$-dimensional vector space $V$ over a finite field $\mathbb{F}$ is finite and has cardinality: 
    \[\left|V\right| = \left|\mathbb{F}\right|^k\]
    
    \subparag{Remark}{
        This theorem works for \textit{any} vector space, including subspaces for instance.
    }

    \subparag{Proof}{
        Let $\left(\bvec{b_1}, \ldots, \bvec{b_k}\right)$ be a basis of $V$. We know that for every $\bvec{v} \in V$, there is a unique $k$-tuple $\left(\lambda_1, \ldots, \lambda_k\right) \in \mathbb{F}^k$ such that: 
        \[\bvec{v} = \sum_{i=1}^{k} \lambda_i \bvec{v_i}\]
        
        Thus, we know that the following mapping is a bijection: 
        \[\begin{split}
        \mathbb{F}^k &\longmapsto V \\
        \left(\lambda_1, \ldots, \lambda_k\right) &\longmapsto \bvec{v} = \sum_{i=1}^{k} \lambda_i \bvec{v_i}
        \end{split}\]
        
        However, this implies that the two sets have the same number of elements. So, by the theorem we have just seen: 
        \[\left|V\right| = \left|\mathbb{F}^k\right| = \left|\mathbb{F}\right|^{k}\]

        \qed
    }
}


\parag{Subspaces}{
    Let $S \subseteq \mathbb{F}^n$, where $\mathbb{F}$ is a finite field. This is described by either:
    \begin{enumerate}
        \item A list of vectors that span $S$.
        \item By a system of linear homogeneous equations.
    \end{enumerate}
}

\parag{Example}{
    Let's consider the vector space $\mathbb{F}_5^3$, and the following subspace: 
    \[S = \left\{\bvec{v} \in \mathbb{F}_5^3 \text{ such that } \bvec{v} = \alpha\left(1, 2, 3\right) \text{ for some } \alpha \in \mathbb{F}_5\right\}\]
    
    Equivalently, we want:
    \[\begin{systemofequations} v_1 = \alpha \\ v_2 = 2\alpha \\ v_3 = 3\alpha \end{systemofequations} \iff \begin{systemofequations} v_2 = 2v_1 \\v_3 = 3v_1 \end{systemofequations} \iff \begin{systemofequations} 2v_1 + \left(-v_2\right) = 0 \\ 3v_1 + \left(-v_3\right) = 0 \end{systemofequations} \iff \begin{systemofequations} 2v_1 + 4v_2 = 0 \\3v_1 + 4v_3 = 0 \end{systemofequations}\]
    since $\left[-1\right]_5 = \left[4\right]_5$.

    This means that we can also write $S$ as: 
    \begin{multiequality}
    S =\ & \left\{\bvec{v} \in \mathbb{F}_5^3 \text{ such that } \begin{systemofequations} 2v_1 + 4v_2 = 0 \\ 3v_1 + 4v_3 = 0 \end{systemofequations}\right\}  \\
    =\ & \left\{\bvec{v} \in \mathbb{F}_5^3 \text{ such that } \bvec{v} \begin{pmatrix} 2 & 4 & 0 \\ 3 & 0 & 4 \end{pmatrix}^T = \bvec{0}\right\} 
    \end{multiequality}
    where $\bvec{v} = \left(v_1, v_2, v_3\right)$ is a row vector.

    This means that we can write: 
    \[S = \left\{\bvec{v} : \bvec{v} A^T = 0\right\} \text{ where } A = \begin{pmatrix} 2 & 4 & 0 \\ 3 & 0 & 4 \end{pmatrix}\]
    
    \subparag{Observation}{
        The equations are linearly independent if and only if the rows of the matrix $A$ are linearly independent vectors.
    }

    \subparag{Remark}{
        Note that, for $\bvec{v}$ being a row vector, and $\bvec{u}$ being the same vector but as a column, we have: 
        \[\left(A \bvec{u}\right)^T = \bvec{v} A^T\]

        Thus we are dealing with regular matrix multiplication, just in the row vector world.
    }
}

\parag{Definition: Rank}{
    The number of linearly independent rows of a matrix is called the \important{rank}. 

    \subparag{Remark}{
        Note that this is equal to the number of linearly independent columns of the matrix.
    }
}

\parag{Definition}{
    A vector $\bvec{v} = \left(v_1, \ldots, v_n\right) \in \mathbb{F}^n$ \important{fulfills a system of linear homogeneous equations} with coefficients in $\mathbb{F}$ if it satisfies $\bvec{v} A^T = \bvec{0}$ where $A$ is the matrix of equations.
}

\parag{Theorem}{
    \begin{itemize}[left=0pt]
        \item The set of solutions in $\mathbb{F}^n$ of a set of $m$ homogeneous linear equations in $n$ variables and coefficients in $\mathbb{F}$ is a subspace $S$ of $\mathbb{F}^n$.
        \item Let $A$ be the coefficient matrix, and $r$ be the rank of $A$. Then, $\dim\left(S\right) = n - r$.
        \item Conversely, if $S$ is a subspace of dimension $\dim\left(S\right) = k$, then there exists a system of $m = n - k$ linear homogeneous equations with coefficients in $\mathbb{F}$, the solution of which is exactly $S$.
    \end{itemize}
}

\parag{Example 1}{
    Let's consider the vector space $\mathbb{F}_7^3$, and the following subspace: 
    \[S = \left\{\bvec{v} : 3v_1 + 2v_2 + v_3 = 0\right\} \implies A = \begin{pmatrix} 3 & 2 & 1 \end{pmatrix} \]
    
    We notice that $\rank\left(A\right) = 1$, thus $\dim\left(S\right) = n - \rank\left(A\right) = 2$. We can find rather easily two linearly independent vectors, which will give us a basis for $S$: 
    \[\bvec{s_1} = \begin{pmatrix} 1 & 0  & s_{13} \end{pmatrix}  \implies s + s_{13} = 0 \implies s_{13} = -3 = 4\]
    
    By a similar argument, we can find a second vector. This gives us the following basis: 
    \[\left\{\begin{pmatrix} 1 & 0 & 4 \end{pmatrix}, \begin{pmatrix} 0 & 1 & 5 \end{pmatrix} \right\}\]
    
}


\end{document}
