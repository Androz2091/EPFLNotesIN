\documentclass[a4paper]{article}

% Expanded on 2022-03-07 at 10:18:06.

\usepackage{../../style}

\title{Analyse 1}
\author{Joachim Favre}
\date{Lundi 07 mars 2022}

\begin{document}
\maketitle

\lecture{5}{2022-03-07}{Wronskien}{
\begin{itemize}[left=0pt]
    \item Définition du Wronskien, et preuve du théorème faisant le lien avec des solutions linéairement indépendantes d'une EDL2 homogène.
    \item Démonstration de la forme des solutions aux EDL2 homogènes.
    \item Démonstration de la méthode de la variation de la constante pour résoudre les EDL2 complètes.
    \item Long exemple de résolution d'une EDL2 complète.
\end{itemize}

}

\subsection{Wronskien}
\parag{Définition: Wronskien}{
    Soient $v_1, v_2 : I \mapsto \mathbb{R}$ deux fonctions dérivables sur $I \subset \mathbb{R}$. Nous appelons la fonction $W\left[v_1, v_2\right] : I \mapsto \mathbb{R}$ définie par:
    \[W\left[v_1, v_2\right] = \det\begin{pmatrix} v_1\left(x\right) & v_2\left(x\right) \\ v_1'\left(x\right) & v_2'\left(x\right) \end{pmatrix} = v_1\left(x\right) v_2'\left(x\right) - v_2\left(x\right) v_1'\left(x\right)\]
    le \important{Wronskien} de $v_1$ et $v_2$.
}


\parag{Exemple}{
    Prenons l'équation suivante:
    \[y'' + 2y' + y = 0\]

    Son polynôme caractéristique, $\lambda^2 + 2\lambda + 1 = 0$ nous donne les racines $a = b = -1$, ce qui nous permet de trouver la solution générale:
    \[v\left(x\right) = C_1e^{-x} + C_2 xe^{-x}, \mathspace \text{où } C_1, C_2 \in\mathbb{R}, \forall x \in \mathbb{R}\]

    Calculons le Wronskien de nos deux solutions:
    \[W\left[e^{-x}, xe^{-x}\right] = \det\begin{pmatrix} e^{-x} & xe^{-x} \\ -e^{-x} & e^{-x} - xe^{-x} \end{pmatrix} = e^{-2x} -xe^{-2x} + xe^{-2x} = e^{-2x}\]

    On observe que $e^{-2x} = W\left[e^{-x}, xe^{-x}\right] \neq 0$ pour tout $x \in\mathbb{R}$. Nous allons généraliser cette idée.
}

\parag{Proposition pour le Wronskien}{
    Soient $v_1, v_2 : I \mapsto \mathbb{R}$ deux solutions de l'équation $y''\left(x\right) + p\left(x\right)y'\left(x\right) + q\left(x\right)y\left(x\right) = 0$ (EDL2 homogène).

    $v_1\left(x\right)$ et $v_2\left(x\right)$ sont linéairement indépendants si et seulement si $W\left[v_1, v_2\right]\left(x\right) \neq 0$ pour tout $x \in I$.

    \demonstrationaconnaitre

    \subparag{Preuve $\impliedby$}{
        Démontrons ce point par la contraposée. Nous voulons donc montrer que les solutions sont linéairement dépendantes implique qu'il existe $x \in I$ tel que $W\left[v_1, v_2\right]\left(x\right) = 0$.

        Puisque nos deux solutions sont linéairement dépendantes, nous pouvons prendre sans perte de généralité qu'il existe $c \in \mathbb{R}$ tel que $v_1\left(x\right) = cv_2\left(x\right)$ (si plutôt $v_2\left(x\right) = c v_1\left(x\right)$, nous pourrions juste échanger les noms, d'où le ``sans perte de généralité'').

        Ainsi, nous avons:
        \begin{multiequality}
        W\left[v_1, v_2\right]\left(x\right) =\ & \det\begin{pmatrix} v_1\left(x\right) & cv_1\left(x\right) \\ v_1'\left(x\right) & cv_1'\left(x\right) \end{pmatrix}  \\
        =\ & cv_1\left(x\right) v_1'\left(x\right) - cv_1\left(x\right) v_1'\left(x\right) \\
        =\ & 0, \mathspace \forall x \in I
        \end{multiequality}

        Nous avons donc trouvé que le Wronskien est nul pour tout $x$ sur cet intervalle, donc il existe bien un $x$ pour lequel il est égal à 0.
    }

    \subparag{Preuve $\implies$}{
        Prouvons aussi cette affirmation par la contraposée. Nous voulons donc montrer que, s'il existe $x_0 \in I$ tel que $W\left[v_1, v_2\right]\left(x_0\right) = 0$, alors $v_1\left(x\right)$ et $v_2\left(x\right)$ sont linéairement dépendantes.

        Puisqu'il existe un tel $x_0 \in I$, nous savons que:
        \[\det\begin{pmatrix} v_1\left(x_0\right) & v_2\left(x_0\right) \\ v_1'\left(x_0\right) & v_2'\left(x_0\right) \end{pmatrix} = 0\]

        Ainsi, le kernel de cette matrice est est non-trivial (il n'est pas de dimension 0), donc il existe un vecteur non nul $\binom{a}{b} \in \mathbb{R}^2$ tel que:
        \[\begin{pmatrix} v_1\left(x_0\right) & v_2\left(x_0\right) \\ v_1'\left(x_0\right) & v_2'\left(x_0\right) \end{pmatrix} \begin{pmatrix} a \\ b \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \end{pmatrix} \]

        Ainsi:
        \[\begin{systemofequations} a v_1\left(x_0\right) + b v_2\left(x_0\right) = 0 \\ a v_1'\left(x_0\right) + b v_2'\left(x_0\right) = 0 \end{systemofequations}\]

    Soit $v\left(x\right) = av_1\left(x\right) + bv_2\left(x\right)$. Alors, $v\left(x\right)$ est une solution de l'équation donnée par la superposition des solutions. De plus, par le système d'équations que nous venons de trouver, nous avons $v\left(x_0\right) = 0$ et $v'\left(x_0\right) = 0$. Par le théorème de l'existence et unicité d'une solution de l'équation $y''\left(x\right) + p\left(x\right)y'\left(x\right) + q\left(x\right)y\left(x\right) = 0$, cette équation admet une seule solution satisfaisant $y\left(x_0\right) = 0$ et $y'\left(x_0\right) = 0$. Puisque la solution triviale $y\left(x\right) = 0 \ \forall x \in I$ satisfait l'équation et les conditions initiale, alors nécessairement:
    \[v\left(x\right) = av_1\left(x\right) + bv_2\left(x\right) = 0, \mathspace \forall x \in I\]

    Puisque $a$ et $b$ ne sont pas les deux nuls, soit nous avons $v_1\left(x\right) = \frac{-b}{a} v_2\left(x\right)$ pour tout $x \in I$, soit nous avons $v_2\left(x\right) = -\frac{a}{b} v_1\left(x\right)$ pour tout $x \in I$ (soit les deux).

    Nous avons donc bien trouvé que $v_1\left(x\right)$ et $v_2\left(x\right)$ sont linéairement dépendantes sur $I$.

    \qed
    }

    \subparag{Idée de la preuve}{
        On démontre que $Q \implies P$ et $P \implies Q$ par la contraposée car $P$ et $Q$ sont des propositions ``négatives'': il est beaucoup plus simple d'avoir une fonction qui est parfois égale à 0, ou deux fonctions qui sont linéairement dépendantes.
    }
}

\parag{Exercice}{
    Considérons les solutions des EDL2 homogènes à coefficients constants $y''\left(x\right) + py'\left(x\right) + qy\left(x\right) = 0$ telle que les racines du polynôme caractéristique sont $a = \bar{b} = \alpha + i\beta \not\in\mathbb{R}$.

    Nous voulons montrer que $W\left[e^{\alpha x} \cos\left(\beta x\right), e^{\alpha x} \sin\left(\beta x\right)\right] \neq 0$ pour tout $x \in \mathbb{R}$. Avec un peu de travail, nous pouvons obtenir que:
    \[W\left[e^{\alpha x} \cos\left(\beta x\right), e^{\alpha x} \sin\left(\beta x\right)\right] = \beta e^{2\alpha x}, \mathspace \forall x \in \mathbb{R}\]

    Or, puisque $\beta \neq 0$ (sinon $a = \alpha + i\beta \in \mathbb{R}$), nous obtenons bien que le Wronskien de ces deux solutions n'est jamais 0, et donc qu'elles sont linéairement indépendantes.
}

\parag{Théorème: Forme des solutions aux EDL2 homogènes}{
    Soient $v_1, v_2 : I \mapsto \mathbb{R}$ deux solutions linéairement indépendantes de l'équation $y''\left(x\right) + p\left(x\right) y'\left(x\right) + q\left(x\right) y\left(x\right) = 0$.

    Alors, la solution générale de cette équation est de la forme:
    \[v\left(x\right) = C_1 v_1\left(x\right) + C_2 v_2\left(x\right), \mathspace C_1, C_2 \in \mathbb{R}, x \in I\]

    \demonstrationaconnaitre

    \subparag{Preuve}{
        Soit $\widetilde{v}\left(x\right)$ une solution de l'équation donnée, et $x_0 \in I$. Alors, nous savons que $\widetilde{v}\left(x_0\right) = a_0 \in \mathbb{R}$ et $\widetilde{v}\left(x_0\right) = b_0 \in \mathbb{R}$.

        Par hypothèse, nous avons deux solutions linéairement indépendantes $v_1, v_2 : I \mapsto \mathbb{R}$. Ainsi, par la caractérisation, nous savons que $W\left[v_1, v_2\right]\left(x\right) \neq 0$ pour tout $x \in I$, ce qui implique que $W\left[v_1, v_2\right]\left(x_0\right) \neq 0$.

        Or, quand le déterminant d'une matrice est non-nul (la matrice est dite \textit{non-dégénérée}), nous savons qu'une équation l'utilisant a une solution unique. Ainsi, nous savons qu'il existe d'uniques constantes $C_1, C_2 \in \mathbb{R}$ telles que:
        \[\begin{systemofequations} C_1 v_1\left(x_0\right) + C_2 v_2\left(x_0\right) = a_0 \\ C_1 v_1'\left(x_0\right) + C_2 v_2'\left(x_0\right) = b_0 \end{systemofequations}\]

        Considérons la fonction $v\left(x\right) = C_1 v_1\left(x\right) + C_2 v_2\left(x\right)$. Nous pouvons voir deux informations. La première est que $v\left(x\right)$ est une solution de l'équation (puisque $v_1\left(x\right)$ et $v_2\left(x\right)$ sont des solutions). La deuxième est que $v\left(x_0\right) = a_0$ et $v'\left(x_0\right) = b_0$.

        Par le théorème de l'existence et unicité d'une solution des EDL2 homogènes satisfaisant des conditions initiales données $v\left(x_0\right)= a_0$ et $v'\left(x_0\right)= b_0$, on a $\widetilde{v}\left(x\right) = v\left(x\right)$ pour tout $x \in I$. Nous avons donc bien montré que notre solution de départ est de la bonne forme.

        \qed
    }
}

\subsection[EDL2 complètes]{Équations différentielles linéaires d'ordre 2 complètes}
\parag{Introduction}{
    Nous considérons maintenant l'équation complète:
\[y''\left(x\right) + p\left(x\right) y'\left(x\right) + q\left(x\right) y\left(x\right) = f\left(x\right)\]
}

\parag{Superposition des solutions}{
    Si $v\left(x\right)$ est une solution de l'équation $y''\left(x\right) + p\left(x\right) y'\left(x\right) + q\left(x\right) y\left(x\right) = f\left(x\right)$ et $u\left(x\right)$ est une solution de l'équation homogène associée, alors $v\left(x\right) + u\left(x\right)$ est une solution de cette équation.

    \subparag{Preuve}{
        Cette preuve est considérée comme triviale, et elle est laissée en exercice au lecteur.
    }
}

\parag{Méthode de la variation de la constante}{
    Nous cherchons une solution particulière de l'équation complète, supposant que nous connaissons deux solutions linéairement indépendantes de l'équation homogène associée: $v_1, v_2 : I \mapsto \mathbb{R}$.

    Prenons l'\textit{Ansatz} $v_0\left(x\right) = c_1\left(x\right) v_1\left(x\right) + c_2\left(x\right) v_2\left(x\right)$, où $c_1\left(x\right)$ et $c_2\left(x\right)$ sont deux fonctions inconnues de classe $C^1$ sur $I$.

    Nous pouvons dériver notre solution présumée:
    \[v_0'\left(x\right) = {\color{red}c_1'\left(x\right) v_1\left(x\right) + c_2'\left(x\right) v_2\left(x\right)} + c_1\left(x\right) v_1'\left(x\right) + c_2\left(x\right) v_2'\left(x\right)\]

    Supposons aussi que le bout en rouge est égal à 0, car cela permet non seulement de diminuer grandement la taille de notre solution, mais aussi de ne pas avoir de $c''\left(x\right)$ (puisque nous les avons prises de classe $C^1$). Notez que nous avons le droit de faire ceci, car nous prenons un Ansatz. Cela peut ne pas fonctionner, mais si cela fonctionne c'est gagné.

    Nous pouvons encore dériver $v_0'$:
    \[v_0''\left(x\right) = c'_1\left(x\right) v_1'\left(x\right) + c_2'\left(x\right) v_2'\left(x\right) + c_1\left(x\right) v_1''\left(x\right) + c_2\left(x\right) v_2''\left(x\right)\]

    Mettons ce que nous avons trouvé dans notre équation $v_0''\left(x\right) + p\left(x\right)v_0'\left(x\right) + q\left(x\right) v_0\left(x\right) = f\left(x\right)$, de manière à trouver des conditions sur $c_1\left(x\right)$ et $c_2\left(x\right)$:
    \begin{multiequality}
    & c_1'\left(x\right) v_1'\left(x\right) + c_2'\left(x\right) v_2'\left(x\right) + {\color{blue}c_1\left(x\right)v_1''\left(x\right)} + {\color{red}c_2\left(x\right)v_2''\left(x\right)}  \\
    & + {\color{blue} p\left(x\right) c_1\left(x\right)v_1'\left(x\right)} + {\color{red} p\left(x\right) c_1\left(x\right) v_2'\left(x\right)}  \\
    & + {\color{blue} q\left(x\right) c_1\left(x\right) v_1\left(x\right)} + {\color{red}q\left(x\right) c_2\left(x\right)v_2\left(x\right)} \\
    & = f'\left(x\right)
    \end{multiequality}

    Puisque $v_1\left(x\right)$ et $v_2\left(x\right)$ sont des solutions de l'équation homogène associée, les termes en rouge sont égaux à 0, et les termes en bleu sont aussi égaux à 0. Nous obtenons donc une grande simplification à notre équation:
    \[c_1'\left(x\right) v_1'\left(x\right) + c_2'\left(x\right) v_2'\left(x\right) = f\left(x\right)\]

    En tout, nous avons deux conditions (celle que nous venons de trouver, et celle que nous avions prise de manière arbitraire):
    \[\begin{systemofequations} c_1'\left(x\right) v_1\left(x\right) + c_2'\left(x\right) v_2\left(x\right) = 0 \\ c_1'\left(x\right) v_1'\left(x\right) + c_2'\left(x\right) v_2'\left(x\right) = f\left(x\right) \end{systemofequations}\]

    Or nous pouvons écrire notre système sous la forme:
    \[\begin{pmatrix} v_1\left(x\right) & v_2\left(x\right) \\ v_1'\left(x\right) & v_2'\left(x\right) \end{pmatrix} \begin{pmatrix} c_1'\left(x\right) \\ c_2'\left(x\right) \end{pmatrix} = \begin{pmatrix} 0 \\ f\left(x\right) \end{pmatrix} \]

    Cependant, c'est un système d'équations sur $c_1'\left(x\right)$ et $c_2'\left(x\right)$. Puisqu'on sait que $v_1\left(x\right)$ et $v_2\left(x\right)$ sont linéairement indépendantes, nous avons $W\left[v_1, v_2\right]\left(x\right) = \det\begin{psmallmatrix} v_1\left(x\right) & v_2\left(x\right) \\ v_1'\left(x\right) & v_2'\left(x\right) \end{psmallmatrix} \neq 0$ pour tout $x \in I$. Ceci nous dit donc que notre matrice est inversible, et que nous pouvons résoudre notre système de cette manière:
    \[\begin{pmatrix} c_1'\left(x\right) \\ c_2'\left(x\right) \end{pmatrix} = \frac{1}{W\left[v_1, v_2\right]\left(x\right)} \begin{pmatrix} v_2'\left(x\right) & -v_2\left(x\right) \\ -v_1'\left(x\right) & v_1\left(x\right) \end{pmatrix} \begin{pmatrix} 0 \\ f\left(x\right) \end{pmatrix} = \frac{1}{W\left[v_1, v_2\right]\left(x\right)} \begin{pmatrix} -v_2\left(x\right) f\left(x\right) \\ v_1\left(x\right) f\left(x\right) \end{pmatrix} \]

    Nous pouvons maintenant intégrer:
    \[c_1\left(x\right) = -\int \frac{f\left(x\right) v_2\left(x\right)}{W\left[v_1, v_2\right]\left(x\right)}dx, \mathspace c_2\left(x\right) = \int \frac{f\left(x\right) v_1\left(x\right)}{W\left[v_1, v_2\right]\left(x\right)}dx\]
    où on supprime les constantes.

     Nous obtenons donc $v_0\left(x\right) = c_1\left(x\right) v_1\left(x\right) + c_2\left(x\right) v_2\left(x\right)$ est une solution de l'équation complète. Nous pouvons même obtenir la solution générale à cette équation:
     \[v\left(x\right) = C_1 v_1\left(x\right) + C_2 v_2\left(x\right) + v_0\left(x\right), \mathspace \text{où } C_1, C_2 \in \mathbb{R}, x \in I\]
}

\parag{Exemple}{
    Nous voulons trouver la solution générale de l'équation suivante:
    \[y''\left(x\right) \underbrace{- \frac{1}{x \left(\log\left(x\right) - 1\right)}}_{p\left(x\right)} y'\left(x\right) \underbrace{+ \frac{1}{x^2 \left(\log\left(x\right) - 1\right)}}_{q\left(x\right)} y\left(x\right) = \underbrace{\log\left(x\right) - 1}_{f\left(x\right)}, \mathspace \text{sur } \left]e, +\infty\right[ \]

    \subparag{Solution particulière à l'équation homogène}{
        Nous essayons de trouver une solution non-nulle de l'équation homogène associée:
        \[y'' - \frac{1}{x \log\left(x\right) - 1} y' + \frac{1}{x^2 \left(\log\left(x\right) - 1\right)} y = 0\]

        On remarque que $y\left(x\right) = x$ est une solution. En effet, $y'\left(x\right) = 1$ et $y''\left(x\right) = 0$, donc:
        \[-\frac{1}{x \left(\log\left(x\right) - 1\right)} + \frac{x}{x^2 \left(\log\left(x\right) - 1\right)} = 0,\mathspace \forall x \in \left]e, +\infty\right[ \]

        Nous avons donc $v_1\left(x\right) = x$ qui est une solution particulière à l'équation homogène associée.
    }

    \subparag{Solution linéairement indépendante}{
        Nous pouvons trouver une solution linéairement indépendante
 à l'équation différentielle homogène associée en prenant:
     \[v_2\left(x\right) = c\left(x\right) v_1\left(x\right), \mathspace \text{où } c\left(x\right) = \int \frac{e^{-P\left(x\right)}}{v_1^2\left(x\right)}dx, P\left(x\right) = \int p\left(x\right) dx\]

     Nous savons que $p\left(x\right) = -\frac{1}{x \left(\log\left(x\right) - 1\right)}$, donc:
     \[P\left(x\right) = - \int \frac{dx}{x\left(\log\left(x\right) - 1\right)} = -\int \frac{d\left(\log\left(x\right)\right)}{\log\left(x\right) - 1} = -\log\left(\log\left(x - 1\right)\right)\]
     où nous ne mettons pas de constante, et où nous n'avons pas besoin de mettre de valeur absolue puisque $x > e$.

     Calculons maintenant $c\left(x\right)$:
     \[c\left(x\right) = \int \frac{e^{-P\left(x\right)}}{v_1^2\left(x\right)} dx = \int \frac{e^{\log\left(\log\left(x\right) - 1\right)}}{x^2} dx = \int \frac{\log\left(x\right) - 1}{x^2} dx\]

     Nous pouvons utiliser la linéarité des intégrales:
     \[c\left(x\right) = - \int \frac{dx}{x} + \int \frac{\log\left(x\right)}{x^2}dx = - \int \frac{dx}{x^2} - \int \log\left(x\right)d\left(\frac{1}{x}\right)\]

     Faisons une intégration par partie:
     \[c\left(x\right) = - \int \frac{d}{x^2} - \frac{\log\left(x\right)}{x} + \int \frac{1}{x} d\left(\log\left(x\right)\right) = - \int \frac{dx}{x^2} - \frac{\log\left(x\right)}{x} + \int \frac{dx}{x^2}\]

     Et ainsi, on a obtenu notre $c\left(x\right)$:
     \[c\left(x\right) = -\frac{\log\left(x\right)}{x}\]

     Et $v_2\left(x\right)$ en découle directement:
     \[v_2\left(x\right) = c\left(x\right)v_1\left(x\right) = - \frac{\log\left(x\right)}{x} x = -\log\left(x\right)\]


     En examen, il est une bonne idée de vérifier que $v_2\left(x\right)$ est bien une solution.
 }

 \subparag{Solution générale équation homogène}{
     Nous pouvons maintenant trouver la solution générale à l'équation homogène associée:
     \[v\left(x\right)= C_1 v_1\left(x\right) + C_2 v_2\left(x\right) = C_1 x + C_2 \log\left(x\right), \mathspace C_1, C_2 \in \mathbb{R}, x \in \left]e, +\infty\right[ \]
 }

 \subparag{Solution particulière équation complète}{
     Nous voulons maintenant trouver une solution particulière à l'équation complète. On sait que $v_{0}\left(x\right) = c_1\left(x\right) v_1\left(x\right) + c_2\left(x\right) v_2\left(x\right)$ est une solution, où:
     \[c_1\left(x\right) = - \int \frac{f\left(x\right) v_2\left(x\right)}{W\left[v_1, v_2\right]\left(x\right)}, \mathspace c_2\left(x\right) = \int \frac{f\left(x\right) v_1\left(x\right)}{W\left[v_1, v_2\right]\left(x\right)} dx\]

     Nous pouvons calculer le Wronskien:
     \[W\left[v_1, v_2\right]\left(x\right) = \det\begin{pmatrix} v_1 & v_2 \\ v_1' & v_2' \end{pmatrix} = \det\begin{pmatrix} x & -\log\left(x\right) \\ 1 & - \frac{1}{x} \end{pmatrix} = \log\left(x\right) - 1\]
     qui est bien différent de 0 pour tout $x$ sur $\left]e, +\infty\right[ $. Nous pouvons maintenant calculer $c_1$ et $c_2$:
     \begin{multiequality}
         c_1\left(x\right) =\ & - \int \frac{\left(\log\left(x\right) - 1\right)\left(- \log\left(x\right)\right)}{\left(\log\left(x\right) - 1\right)} dx = \int \log\left(x\right)dx  \\
         =\ & x \log\left(x\right) - \int x \frac{1}{x} dx = x\log\left(x\right) - x
     \end{multiequality}
     \[c_2\left(x\right) = \int \frac{\left(\log\left(x\right) - 1\right) x}{\log\left(x\right) - 1} dx = \int x dx = \frac{1}{2}x^2\]
     (où on supprime les constantes).

     Ainsi, nous trouvons une solution particulière à l'équation complète:
     \begin{multiequality}
     v_0\left(x\right) =\ & c_1\left(x\right) v_1\left(x\right) + c_2\left(x\right)v_2\left(x\right)  \\
     =\ & x\left(\log\left(x\right) - 1\right)x + \frac{1}{2}x^2 \left(-\log\left(x\right)\right)  \\
     =\ & \frac{1}{2}x^2 \log\left(x\right) - x^2
     \end{multiequality}
     avec $x \in \left]e, \infty\right[$.

     Encore une fois, en examen, c'est une bonne idée de vérifier notre résultat.
 }

 \subparag{Solution générale équation complète}{
     Nous savons que la solution générale de l'équation complète est donnée par:
     \[v\left(x\right) = C_1 x + C_2 \log\left(x\right) + \frac{1}{2} x^2 \log\left(x\right) - x^2, \mathspace C_1, C_2 \in \mathbb{R}, x \in \left]e, +\infty\right[ \]
 }
}

\end{document}
