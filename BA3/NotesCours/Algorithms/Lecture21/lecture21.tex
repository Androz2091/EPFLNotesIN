% !TeX program = lualatex
% Using VimTeX, you need to reload the plugin (\lx) after having saved the document in order to use LuaLaTeX (thanks to the line above)

\documentclass[a4paper]{article}

% Expanded on 2022-12-05 at 14:17:05.

\usepackage{../../style}

\title{Algorithms}
\author{Joachim Favre}
\date{Lundi 05 d√©cembre 2022}

\begin{document}
\maketitle

\lecture{21}{2022-12-05}{Stochastic probabilistic randomness}{
\begin{itemize}[left=0pt]
    \item Explanation of Dijkstra's algorithm for finding a shortest path in a weighted graph.
    \item Introduction to probabilistic analysis, and definition of indicator random variables.
\end{itemize}

}

\begin{filecontents*}[overwrite]{Dijkstra.code}
procedure Dijkstra(G, w, s):
    InitSingleSource(G, s)
    let S be an empty set
    let Q be an empty priority queue
    insert all G.V into Q
    while !Q.isEmpty():
        u = ExtractMin(Q)
        S = union(S, u)
        for each v in G.Adj[u]:
            Relax(u, v, w)
\end{filecontents*}


\parag{Dijksta's algorithm}{
    Let's consider a much faster algorithm, which only works with non-negative weights. 

    The idea is to make a version of BFS working with weighted graphs. We start with a set containing the source $S = \left\{s\right\}$. Then, we greedily grow the source $S$ by iteratively adding to $S$ the vertex that is closest to $S$ ($v \not\in S$ such that it minimises $\min_{u \in S} u.d + w\left(u, v\right)$). To find those, we use a priority queue. At any time, we have found the shortest path of every element in $S$.

    \subparag{Implementation}{
        The program looks like Prim's algorithm, but we are minimising $u.d + w\left(u, v\right)$ instead of $w\left(u, v\right)$:
        \importcode{Dijkstra.code}{pseudo}
        \later{update code from slides}
    }

    \subparag{Analysis}{
        Just like Prim's algorithm, the running time is dominated by operations on the queue. If we implement it using a binary heap, each operation takes $O\left(\log\left(V\right)\right)$ time, meaning that we have a runtime of $O\left(E \log \left(V\right)\right)$.

        Note that we can use a more careful implementation of the priority queue, leading to a runtime of $O\left(V \log\left(V\right) + E\right)$. This is almost as efficient as BFS.
    }
    
    \subparag{Proof}{
        The proof of optimality of this algorithm is considered trivial and left as an exercise to the reader. The idea is to show the loop invariant stating that, at the start of each iteration, we have for all $v \in S$ that the distance $v.d$ from $s$ to $v$ is equal to the shortest path from $s$ to $v$. This naturally uses the assumption that all weights are non-negative (positive or zero).
    }
}

\section{Probabilistic analysis}
\subsection{Introduction}
\parag{Goal}{
    We see that the worst case usually does not happen, so doing average case and amortised analysis seems usually better. In fact, using randomness on the input may allow us to get out of the worst case and lend in the average case: randomising the elements in an array before sorting it allows avoid the reverse-sorted case.

    Also, randomness is really important in cryptography.

    \subparag{Remark}{
        Getting randomness (or random-looking sequences) is a non-trivial task for computers. We will not consider this problem here.
    }
}


\parag{Hiring problem}{
    Let's say we have $n$ persons entering one after the other, and we want to hire tall people. Our strategy is that, when someone comes in, we see if he or she is taller than the tallest person we had hired before. We want to know how many people we will have hired at the end on average.

    For instance, if $n = 3$ people enter with sizes $2, 1, 3$, then we will hire $2$ and $3$, and thus have hired 2 people.

    \subparag{Unsatisfactory answer}{
        First, we notice that, in the worst case, we can hire $n$ person if they come from the shortest to the tallest. In the contrary, in the best case, we hire $1$ person if they come from the tallest to the shortest.

        However, we realise that we should expect them to enter in uniform random order. Listing all $n!$ possibilities for $n = 3$, we get an expected value of: 
        \[\frac{3 + 2 + 2 + 2 + 1 + 1}{3!} = \frac{11}{6}\]
        
        The thing is, listing $n!$ possibilities is really not tractable as $n$ grows. So, we will need to develop a more intelligent theory.
    }
}

\parag{Theorem: Linearity of expected values}{
    Expected values are linear. In other words, for $n$ random variables $X_1, \ldots, X_n$ and constants $\alpha_1, \ldots, \alpha_n$:
    \[\exval\left(\alpha_1 X_1 + \ldots + \alpha_n X_n\right) = \alpha_1 \exval\left(X_1\right) + \ldots + \alpha_n\exval\left(X_n\right)\]
    
    \subparag{Remark}{
        This result is really incredible since it works for any $X_1, \ldots, X_n$ even if they are absolutely not independent.
    }
}

\parag{Definition: Indicator Random Variables}{
    Given a sample space (the space of all possible outcomes) and an event $A$, we define the \important{indicator random variable} to be:
    \begin{functionbypart}{I\left(A\right)}
    1, \mathspace \text{if $A$ occurs}  \\
    0, \mathspace \text{if $A$ does not occurs}
    \end{functionbypart}
}

\parag{Theorem}{
    Let $X_A = I\left(A\right)$ for some event $A$.

    Then, the expected value of $X_A$ is the probability of the event $A$ to occur, i.e:
    \[\exval\left(X_A\right) = \prob\left(A\right)\]
}


\parag{Example: Coin flip}{
    Let's say we throw a coin $n$ times. We want to know the expected number of heads.

    We could compute it using the definition of expected values: 
    \[\exval\left(X\right) = \sum_{k=0}^{n} k \prob\left(X = k\right)\]
    
    This is the binomial distribution, so it works very well, but we can find a much better way.

    Let's consider a single coin. Our sample space is $\left\{H, T\right\}$ where every event has the same probability to occur. Let us take an indicator variable $X_H = I\left(H\right)$, which counts the number of heads in one flip. Since $\prob\left(H\right) = \frac{1}{2}$, we get that $E\left(X_H\right) = \frac{1}{2}$ by our theorem. 

    Now, let $X$ be a random variable for the number of heads in $n$ flips, and let $X_i = I\left(\text{$i$th flip result}\right)$. By the linearity of the expectations, we get: 
    \[\exval\left(X\right) = \exval\left(X_1 + \ldots + X_n\right) = \exval\left(X_1\right) + \ldots + \exval\left(X_n\right) = \frac{1}{2} + \ldots + \frac{1}{2} = \frac{n}{2}\]
    as expected (lolz).
}

\parag{Back to hiring problem}{
    Let's consider again the hiring problem. We want to do a similar analysis as for the coin flip example.

    Let $H_i$ be the event that the candidate $i$ is hired, and let $X_i = I\left(H_i\right)$. We need to find this expected value and, to do so, we need $\prob\left(H_i\right)$. There are $i!$ ways for the first $i$ candidates to come, and only $\left(i-1\right)!$ where the $i$\Th is the tallest (we set the tallest to come last, and compute the permutations of the $\left(i-1\right)$ left).. Since people are hired if and only if they are the tallest, it gives us: 
    \[\exval\left(X_i\right) = \prob\left(H_i\right) = \frac{\left(i-1\right)!}{i!} = \frac{1}{i}\]
    
    Thus, we find that: 
    \[\exval\left(X\right) = \exval\left(X_1 + \ldots + X_n\right) = \exval\left(X_1\right) + \ldots + \exval\left(X_n\right) = \frac{1}{1} + \ldots + \frac{1}{n} = H_n\]
    
    We know that the harmonic partial series has a complexity of $\log\left(n\right) + O\left(1\right)$. However, the best case is $1$ with probability $\frac{1}{n}$ (if the tallest is the first), and the worst case is $n$ with probability $\frac{1}{n!}$ (if they come in sorted order, from shortest to tallest). By randomising our input before hiring people, it allows us to avoid malicious users to input the worst case and to (almost with certainty) land in the average-case scenario of $\log\left(n\right) + O\left(1\right)$.
}


\end{document}
