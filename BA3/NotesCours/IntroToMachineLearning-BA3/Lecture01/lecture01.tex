% !TeX program = lualatex
% Using VimTeX, you need to reload the plugin (\lx) after having saved the document in order to use LuaLaTeX (thanks to the line above)

\documentclass[a4paper]{article}

% Expanded on 2022-10-06 at 16:08:21.

\usepackage{../../style}

\title{Machine learning}
\author{Joachim Favre}
\date{Mercredi 20 septembre 2022}

\begin{document}
\maketitle

\lecture{1}{2022-09-20}{We're just drawing a line}{
\begin{itemize}[left=0pt]
    \item Explanation of vocabulary such as data sample, data set, unsupervised data, supervised data, unsupervised learning and supervised learning.
    \item Explanation of the difference between regression and classification.
    \item Introduction to one-dimensional linear regressions.
\end{itemize}

}

\section{Introduction}
\parag{Goal}{
    The goal of machine learning is to mimic the way human learns: learn from data we collect from the world. 

    This can be useful to better understand some phenomenons (such as by identifying the mother's characteristics that lead to low birth weight), to better understand some data (for instance, to analyse the influences between different musical genres), and even to make concrete predictions (such as knowing what object is on a picture).
}

\parag{Data}{
    Data can have many forms. It can be numbers, text, sound, and so on.

    We make the distinction between a \important{data sample} (an individual observation, such as a list of attributes for one patient), and a \important{data set} (a collection of multiple data samples, such the attribute lists for multiple patients).

    \important{Unsupervised data} is data without information about its content (for instance, the image of a number without the number it represents). \important{Supervised data} is data where we have additional annotations (such as a category label).
}

\parag{Regression and classification}{
    In this course, we will focus on two class of machine learning problems: regression and classification. 

    The goal of \important{regression} is to predict continuous values for a given sample (such as predicting the birth weight of a baby, or estimating the position of a human's arm from a photo). The goal of \important{classification} is to predict some discrete labels for a given sample (such as saying if there is a cat, a tree or a car on a picture).

    The main difference between those two classes is that, in regression, the values follow an order (meaning that predicting 3002 instead of 2977 is better than predicting 2500), whereas in classification the categories do not follow any order (if we predict ``tree'' instead of ``cat'', we are just as wrong as if we had predicted ``car'').
}

\parag{Supervised and unsupervised learning}{
    In this course, we will focus on two main classes of algorithms: \important{supervised} and \important{unsupervised} learning. In the latter, we rely on supervised data and we would like to get the annotations for new samples. In the former, we rely on unsupervised data, and we want to analyse the observed data set.

    Note that, when doing supervised learning, we use data to train our algorithm, and we can then use it to predict the output for a new data sample. We will want to assess how well our algorithm works and, to do so, we will use test data. The training set and the test set should always be completely separate: we must never use the test annotations during training.

    There exists other classes of algorithms, such as reinforcement learning, where we want the algorithm to learn by reacting to the environment. However, this will not be covered in this course.
}

\section{Linear model}
\subsection{Single dimension output}
\parag{Introduction}{
    The linear model is a simple supervised learning model.   
}

\parag{Notation}{
    We denote the $i$\Th data sample (input) in the collection of our $N$ samples as:
    \[\bvec{x_i} \in \mathbb{R}^D\]
    
    We denote the $i$\Th label (output) in the collection of $N$ samples as $y_i$. For classification, this represents the category the samples belongs to. For regression, $\bvec{y_i} \in \mathbb{R}^C$ is a single continuous value or a column vector of dimension $C$.

    
    \subparag{Example}{
        Let's say we have 1000 grey-scale $50\times50$ images representing a digit, each with the digit they represent.

        Then, $\bvec{x_i}$ is the picture vectorised (we take every column of pixels of the picture and stack them in a big vector), $y_i$ is a single discrete value indicating the digit, the dimensionality of the input is $C = 50\cdot 50 = 2500$, the dimensionality of the output is $D = 1$ and we have $N = 1000$ samples.
    }
    
}

\parag{Linear regression}{
    For now, we will let $D = C = 1$. 

    We know that, mathematically, a line is expressed as $y = w^{\left(0\right)} + w^{\left(1\right)}x$. Thus, given $N$ pairs $\left\{\left(x_i, y_i\right)\right\}$, we want to find the best line that passes through these observations, the one that best fits the training samples. This process is named \important{linear regression}.
    \imagehere[0.4]{LinearRegression.png}

    In other words, given our training pairs $\left\{\left(x_i, y_i\right)\right\}$, we aim to find $\left(w^{\left(0\right)}, w^{\left(1\right)}\right)$, such that the predictions of the model $\hat{y_i} = w^{\left(0\right)} + w^{\left(1\right)}x_i$ are as close to the true values $y_i$.

    We first need to define a measure of closeness (or conversely of error) between $y_i$ and $\hat{y_i}$.

    \subparag{Example}{
        1D linear regression can for instance be used to predict the price of a house based on its size.
    }
    
    \subparag{Personal remark}{
        Note that we must be careful when applying a model (especially when we did not train it on much data). Here is a related XKCD:
        \imagehere[0.7]{xkcd-extrapolating.png}
        \begin{center}
            \url{https://xkcd.com/605/}
        \end{center}
        

    }
    
}

\parag{Euclidean distance}{
     A natural measure of error is the Euclidean distance:
     \[d\left(\hat{y_i}, y_i\right) = \sqrt{\left(\hat{y_i} - y_i\right)^2}\]

     In practice, one often prefers using the squared Euclidean distance:
     \[d ^2\left(\hat{y_i}, y_i\right) = \left(\hat{y_i} - y_i\right)^2\]

     Indeed, we will want to compute the gradient of this distance, and absolute values are not smooth around 0. Also, having a square root is a complication since it tends to stay after the gradient computation and makes it really harder (and even impossible often) to find a closed form.
     
     \subparag{Remark}{
         Making linear regression using the squared Euclidean distance is named the \important{least-squares method}.
     }
}

\parag{Prediction}{
    Once we have found the best line for our given $N$ observation, we can use it to predict $y$ values for a new $x$. 

    Let's say $w^{\left(0\right)*}$ and $w^{\left(1\right)*}$ are the best line parameters given the observations. Then, for any value $x$, we can predict an estimate of the corresponding $y$ as:
    \[\hat{y} = w^{\left(0\right)*} + w^{\left(1\right)*} x\]
    
}

\end{document}
