% !TeX program = lualatex
% Using VimTeX, you need to reload the plugin (\lx) after having saved the document in order to use LuaLaTeX (thanks to the line above)

\documentclass[a4paper]{article}

% Expanded on 2022-09-27 at 13:13:14.

\usepackage{../../style}

\title{Numet}
\author{Joachim Favre}
\date{Mardi 27 septembre 2022}

\begin{document}
\maketitle

\lecture{2}{2022-09-27}{LU biscuits}{
\begin{itemize}[left=0pt]
    \item Explanation on how to measure error for algorithms.
    \item Explanation on how and when to use the LU decomposition.
\end{itemize}

}

\begin{filecontents*}[overwrite]{FloatingPointError.code}
import numpy as np
a = 1e16
b = 1

print(a - a + b) # 1
print(a + b - a) # 0, so not commutative

a = np.float32(0.1234567000000000)
b = np.float32(0.1234567123456789)
print(b-a)  # 7.450581e-09, so not at all what was expected (would be correct for np.float64)
\end{filecontents*}

\begin{parag}{Floating point error}
    As mentioned in the last course, we have to be careful about floating point errors. We can see two in the following code snippet:
    \importcode{FloatingPointError.code}{python}
\end{parag}

\begin{parag}{Error measuring}
    We can measuring in two ways: 
    \[\text{absolute error} = \text{approximate value} - \text{true value}\] 
    \[\text{relative error} = \frac{\text{absolute error}}{\text{true value}}\]

    In general, when measuring the error for floating points, it is better to consider relative error. If we have a sequence making $a$ and $b$ getting closer and closer, and we want to loop until $\left|a - b\right| < c$, then we might have problems if $a$ and $b$ are very large: the ULP might be bigger than $c$, meaning that this cannot be reached unless $a = b$. 
\end{parag}

\begin{parag}{Forward and backward error}
    Due to numerical errors because of approximation and errors, the actual algorithm does not always lead to the exacte result we want. 

    Let's say that we have an algorithm taking input $x$ that would work with infinite precision, giving $y = f\left(x\right)$. Let's say that our real algorithm, introducing approximation, is $\hat{y} = g\left(x\right)$. The \important{forward error} is defined to be the error between the expected and the actual value, $\Delta y = y - \hat{y}$. The \important{backward error} is defined to be the smallest $\Delta x$ such that $f\left(x + \Delta x\right) = \hat{y}$, meaning the distance between the problem we wanted to solve and the one we actually solved. This can be represented as:
    \imagehere[0.7]{ForwardBackwarError.png}

    We also use those definitions to define the \important{condition number}:
    \[\text{condition number} = \frac{\text{forward error}}{\text{backward error}}\]

    A large condition number means that the algorithm is very sensitive to small errors in the input (it is said to be poorly conditioned), whereas a small condition number means having a high quality input does not produce garbage at the end (such algorithm is said to be well-conditioned). This is for instance important for data we took from experiments in physics: such data cannot be perfect.
\end{parag}



\section{Linear systems}
\begin{parag}{Remark}
    We have very good tools to solve linear systems, so if we can explain our algorithm as a linear system, then we have solved our problem.
\end{parag}

\subsection{LU decomposition}
\begin{parag}{Restriction}
    For now, we will only restrict ourselves to square invertible matrice.
\end{parag}

\begin{parag}{Remark}
    Note that there are two ways to compute $A^{-1} \bvec{b}$. The first one is to compute the inverse matrix, and then multiply. However, this is very inefficient, because computing $A^{-1}$ is very slow and can be poorly conditioned.

    The best way is often, in fact, to solve $A \bvec{x} \bvec{b}$ by using a decomposition such as the LU. This does give us $\bvec{x} = A^{-1} \bvec{b}$. Note that, in this course, when we see an inverse matrix, this often means that we need to compute values through this second method (and thus not compute the inverse matrix).
\end{parag}

\begin{filecontents*}[overwrite]{gaussianElimination.code}
// Elimination, O(n^3)
for column k in [1, ..., n]:
    select pivot row i, scale to make pivot element equal to 1
    for each row j > 1, substract multiple of row i to create zeros in column k  // another loop over elements

// Back-substitution, O(n^2)
for row i in [n, ..., 1]:
    for each j < i, zero out column entries above
\end{filecontents*}


\begin{parag}{Gaussian elimination}
    We consider a nonsingular matrix (meaning it has an non-zero determinant).

    The goal of Gaussian elimination is that, using three laws, we want to reach an echeloned matrix. The three laws are permuting two rows, adding the multiple of a row to another and multiplying a row by a constant non-zero value.

    We can write it as the following algorithm:
    \importcode{gaussianElimination.code}{pseudo}
    
    We can see in complexities that solving a triangular system is very easy: we have already done the hard work. 

    \begin{subparag}{Complexities}
        Let's take a bit more look at the runtime complexities, and compare them to computing $\bvec{x}$ through $\bvec{x} = A^{-1} \bvec{b}$.

        Computing $\bvec{x}$ through the inverse takes $n^3$ operations to compute the inverse (which can be numerically unstable), followed by $2n^2$ operations for the matrix-vector multiplication. However, the algorithm we just saw takes $\frac{2}{3} n^3$ computations to do the Gaussian elimination, followed by $n^2$ computations for the triangular solve. Computing $x \bvec{x} = A^{-1} $

        In other words, as mentioned earlier, Gaussian elimination is faster than inverting $A$.
    \end{subparag}
    

    \begin{subparag}{Issues}
        There can be instabilities which can occur when pivot elements are tiny (this is a problem since we would need to divide by this value). Thus, we choose the pivot element which is the largest to avoid those problems. There are two algorithms doing this idea.

        When we do \important{partial pivoting}, at every iteration, we select the number with the highest absolute value in the column (in red in the picture hereinbelow) by swapping lines. When we do \important{full pivoting}, we select the highest absolute value in the sub-matrix (in green in the picture hereinbelow) by swapping lines and column (when we swap columns, we have to remember which we swapped, since it is equivalent to swapping two variable names).
        \imagehere{PartialFullPivoting.png}

        Note that, often, partial pivoting is enough.
    \end{subparag}
    
\end{parag}

\begin{parag}{Common use case}
    We sometimes have to compute systems which have the same matrix $A$ but a different $\bvec{b}$. Doing Gaussian elimination over and over again seems very suboptimal, and so as for computing the inverse (for the same reasons as before).

    We can realise that none of the decisions taken by the Gaussian algorithm depends on the $\bvec{b}$ vector. Thus, we can record the involved row operations. However, since the operations are linear, they can always be represented by matrix multiplications. We can convince ourselves that the used matrix is the identity matrix, but with the operation we want to apply applied on it. For instance, adding twice the first row to the second row would be represented by the matrix:
    \[\begin{pmatrix} 1 & 0 & 0 \\ 2 & 1 & 0 \\ 0 & 0 & 1 \end{pmatrix} \]
    
    Thus, our Gaussian elimination can be sumarised as: 
    \[U = E_n \cdots E_1 A\]
    where $U$ is an upper-triangular matrix. We can modify our system to get: 
    \[A = \left(E_n \cdots E_1\right)^{-1} U = \underbrace{E_1^{-1} \cdots E_n^{-1}}_{= L} U = LU\]

    In fact, if we restrict ourselves only to using the ``add a multiple of a line to another line'' rule for Gaussian elimination, then we realise that it always gives us a lower-unitriangular (it has only ones on its diagonal) matrix $L$. Also, we are not really computing inverses; the operations we are doing are so simple that we can fill in the matrices $L$ and $U$ while running Gaussian elimination.

    To solve our system, we can compute: 
    \[x = U^{-1} \left(L^{-1} \bvec{b}\right) \iff L U \bvec{x} = \bvec{b}\]
    which can be solved very easily since triangular systems only take $O\left(n^2\right)$ to be computed, and we only need to compute $L \bvec{y} = \bvec{b}$, followed by $U \bvec{x} = \bvec{y}$.
\end{parag}

\begin{parag}{Pivoting}
    As mentioned before, when doing partial and full pivoting, we need to remember which permutations we did. To do so, we can use permutation matrices, matrices $P$ such that $P ^{-1} = P^T$. For instance, we could have:
    \[P = \begin{pmatrix} 1 & 0 & 0 \\ 0 & 0 & 1 \\ 0 & 1 & 0 \end{pmatrix} \]
    
    Permuting rows is equivalent to pre-multiplying by a permutation matrix, and permuting columns is equivalent to post-multiplying by a permutation matrix.

    As mentioned before, when computing the LU decomposition, we are only allowed to add a multiple of a line to another line. However, we would really like to be able to swap rows in order to do partial pivoting. We thus can pre-multiply by a permutation matrix:
    \[A = PLU\]
    
    Also, when doing full pivoting, we need to keep in mind which columns we switched, which is done by post-multiplying by a permutation matrix.
    \[A = P_1 LU P_2\]
\end{parag}

\begin{parag}{Memory}
    When implementing LU matrix decomposition, we can do some memory optimisations. First, storing the permutations in a matrix is very inefficient, we can instead store them in a vector.

    Also, we can notice that the $L$ matrix is a lower unitriangular matrix, thus we only care about its elements strictly under the diagonal, and $U$ is an upper triangular matrix, thus we only care about its elements which are on the diagonal or above. This means that we can store both the $L$ and $U$ matrix in a common $LU$ $\left(n \times n\right)$-matrix (its elements strictly under the diagonal give the ones of $L$, and the others the ones of $U$). Note that this matrix does not represent anything, it is just used to decrease memory use. 
\end{parag}


\begin{filecontents*}[overwrite]{SolveLinearSystem.code}
import scipy

# Must not do (unless good reason)
x = scipy.linalg.inv(A) @ b

# Must almost always do
lu, piv = scipy.linalg.lu_factor(A)
# lu is both L and U stored in same matrix; piv is a vector storing pivots
x = scipy.linalg.lu_solve((lu, piv), b)
\end{filecontents*}

\begin{parag}{Python}
    In python, to compute solve a system through LU decomposition, we can write:
    \importcode{SolveLinearSystem.code}{python}
\end{parag}

\begin{parag}{Remark}
    The goal is not to understand completely the algorithm and to be able to write one (since it would probably be less efficient than the one provided in python libraries anyway), but more to understand how and when to apply it.
\end{parag}


\begin{parag}{Special}
    Both work and storage can be saved if we know a bit more about the linear system:
    \begin{itemize}
        \item Symmetric matrix: $A = A^T$
        \item Positive definiteness: $\bvec{x}^T A \bvec{x} > 0$ for all $\bvec{x} \neq \bvec{0}$
        \item Banded matrix: $A_{ij} = 0$ for all $\left|i - j\right| > \beta$ for some $\beta$ (basically there are only elements on and around the diagonal).
        \item Sparse matrix: contains almost only zeroes. Those are a very big and important topic.
    \end{itemize}

    \begin{subparag}{Example: Cholesky factorisation}
        If we have a symmetric positive definite matrix, then we can compute 2 times quicker the Cholesky factorisation: 
        \[A = L L^T\]
    \end{subparag}
    
\end{parag}


\end{document}
