% !TeX program = lualatex
% Using VimTeX, you need to reload the plugin (\lx) after having saved the document in order to use LuaLaTeX (thanks to the line above)

\documentclass[a4paper]{article}

% Expanded on 2022-10-25 at 13:17:51.

\usepackage{../../style}

\title{NumMet}
\author{Joachim Favre}
\date{Mardi 25 octobre 2022}

\begin{document}
\maketitle

\lecture{6}{2022-10-25}{Eigenthings and Swiss army knife}{
\begin{itemize}[left=0pt]
    \item Explanation of eigendecomposition and of its uses.
    \item Application of the eigendecomposition to dimensionality reduction and to the solving of systems of ordinary differential equations.
    \item Explanation of the SVD, and of many of its applications.
    \item Explanation on how to use a SVD to compute a regularised inverse (or pseudo-inverse) of a matrix.
\end{itemize}

}

\subsection{Eigendecomposition}
\parag{Eigenthings}{
    For any square matrix $A \in \mathbb{R}^{n \times n}$, there sometimes exist some vectors $\bvec{v} \in \mathbb{R}^{n}$ and some constant $\lambda \in \mathbb{R}$ such that: 
    \[A \bvec{v} = \lambda \bvec{v}\]
    
    $\lambda$ is named the \important{eigenvalue} and $\bvec{v}$ is named the eigenvector. Since we have infinitely many $\bvec{v}$ for a given $\lambda$, we usually take $\bvec{v}$ such that $\left\|\bvec{v}\right\| = 1$.

    \subparag{Ellipse}{
       As before, we can summarise what a matrix does by applying it to every point of a unit circle, and getting an ellipse:
       \imagehere[0.5]{EigenvectorsEllipse.png}
        
       Then, if a vector is an eigenvector, then it must be at an intersection of the circle and the ellipse.
    }
    
}

\parag{Theorem}{
    If a matrix $A$ is non-singular, then there are exactly $n$ distinct eigenvalue-eigenvectors pairs, and the eigenvectors $\bvec{v}_i$ form a basis for $\mathbb{R}^n$ (which is not necessarly orthogonal).
}

\parag{Eigendecomposition}{
    Let's put our eigenvectors and eigenvalues in a matrix:
    \[V = \begin{pmatrix}  &  &  \\ \bvec{v}_1 & \cdots & \bvec{v}_n \\  &  &  \end{pmatrix}, \mathspace \Lambda = \begin{pmatrix} \lambda_1 & \cdots & 0 \\ \hdots & \ddots & \hdots \\ 0 & \cdots & \lambda_n \end{pmatrix} \]

    Then, we can see that: 
    \[A V = V \Lambda \iff A = V \Lambda V^{-1}\]

    \subparag{Geometric intuition}{
        The eigendecomposition expresses a linear transformation $A$ as three step: we change the basis to a system $\mathcal{V}$ with $V^{-1}$, apply a non-uniform scaling $\Lambda$, and then get back to our original coordinate system by applying $V$.
    }

    \subparag{Properties}{
        We see that this is very useful for applying powers: 
        \[A^k = V \Lambda^k V^{-1}\]
        
        Also, in principle, we can compute the inverse of the matrix cheaply (ignoring the fact that we need $V^{-1}$): 
        \[A^{-1} = V\Lambda^{-1} V^{-1}\]
    }
}


\begin{filecontents*}[overwrite]{PowerIterationDominantEigenvector.code}
x = np.random.random(size)
for i in range(num_iterations):
    x = A @ x
    x /= np.linalg.norm(x)  # we only want its direction
\end{filecontents*}

\parag{Compute an eigendecommposition}{
    In maths, when we compute eigenvalues, we solve for the root of the following plynomial: 
    \[p\left(\lambda\right) = \det\left(A - \lambda I\right) = 0\]
    
    However, it is not always possible to find a closed form for polynomials of degree of 5 or more. Solving this is a nonlinear operation, meaning that we will get an approximate result and that we have no guarantees on the computation time.

    \subparag{Power iteration}{
        Instead of searching root for a polynomial, let us make a better algorithm. Saying that $\lambda_1 > \ldots > \lambda_n$, we get that, as $k \to \infty$: 
        \[\Lambda^k \approx \begin{pmatrix} \lambda_1^k &  &  &  \\  & 0 &  &  \\  &  & \ddots &  \\  &  &  & 0 \end{pmatrix} \]
        where the eigenvalues are ordered by convention, meaning that $\lambda_1$ is the biggest eigenvalue.

        So, applying $A^k = V \Lambda^k V^{-1}$ to a random vector, we make a change of variables to make it go to the eigen-vector space, flatten it on the main axis, and turn it back into the first space; giving us the direction of the eigenvector with biggest eigenvalue:
        \importcode{PowerIterationDominantEigenvector.code}{pseudo}
        
        This algorithm is named \important{power iteration}, and allows to compute the dominant eigenvector. Also, it may not always converge (for instance if the first two eigenvalues are very close to each other)
    }
    
    \subparag{Inverse iteration}{
        To find the smallest eigenvalue, we can use our algorithm above on $A^{-1}$, since, now, as $k \to \infty$, the biggest eigenvalue $\frac{1}{\lambda_n}$ dominates, which is the smallest eigenvalue of $A$. This is named \important{inverse iteration}.
    }

    \subparag{In practice}{
        In practice, those algorithms are much more complex. We run many versions of power iteration at the same time, and we ``push vectors apart'' to keep them from converging to the same eigenvector-eigenvalue pair.
    }
}

\parag{Application: Dimensionality reduction}{
    Let's say we have a lot of data in which we try to find a correlation. However, we have data on things that relate a lot (growth of plant and the amount of ferilizer), and other that don't at all (the two previous with the average temperature on the back side of the moon and the EPFL electricity bill).

    We thus want to through useless dimensions to only keep the ones that are useful. 

    \subparag{2D}{
        Let's begin to 2D. We want to find a line to which we could project all our point. Note that this may look similar to linear regression, but this is not (since we are computing the distance with the projection, not only in the $y$-value).

        Our goal is to minimise: 
        \[\sum_{i=1}^{n} \left\|\bvec{x}_i - \proj_{\bvec{v}}\left(\bvec{x}_i\right)\right\|_2^2\]
        where $\proj_{\bvec{v}}\left(\bvec{x}_i\right) = \left\langle \bvec{x}_i, \bvec{v} \right\rangle \bvec{v}$, assuming that $\left\|\bvec{v}\right\| = 1$.

        We can then see, this is equivalent to minimising:
        \[\sum_{i=1}^{n} -\left\langle \bvec{x}_i, \bvec{v} \right\rangle^2 = - \left\|X^T \bvec{v}\right\|_2^2 = -\bvec{v}^T X X^T \bvec{v}\]
        where $X$ is a matrix which rows are the vectors $\bvec{x}_1, \ldots, \bvec{x}_n$.

        We want to maximise the opposite $\bvec{v}^T X X^T \bvec{v}$, and we can show that the $\bvec{v}$ which maximises this expression is the eigenvector associated with the largest eigenvalue of the matrix $X X^T$.
    }

    \subparag{Remark}{
        Note that, in real life, dimensionality reduction through eigenvalues is not used since we have better algorithms.

        Indeed, the big problem is that we have $\cond\left(X X^T\right) \approx \cond\left(X\right)^2$. So, using our algorithm would lead to many numerical instabilities.
    }
}

\parag{Application: Solve ordinary differential equations}{
    We know that the equation $x'\left(t\right) = ax\left(t\right)$ has the solution $x\left(t\right) = e^{at} x\left(0\right)$.

    Thus, it would make sense to have the same idea for systems of differential equations. If we have $\bvec{x}'\left(t\right) = A \bvec{x}\left(t\right)$, then the solution could be of the form: 
    \[\bvec{x}\left(t\right) = e^{A t} \bvec{x}\left(0\right)\]
    
    We would have to define $e^{At}$ in such a way that its derivative is $Ae^{At}$ for this to work. We know the standard definition of the exponentiation function, which is one of the explanation of why the derivative of the exponential is equal to itself: 
    \[e^a = \sum_{k=0}^{\infty} \frac{1}{k!} a^k\]
    
    Now, plugging in a matrix instead: 
    \[e^{A} = \sum_{k=0}^{\infty} \frac{1}{k!} V \Lambda^k V^{-1} = V \left[\sum_{k=0}^{\infty} \frac{1}{k!} \Lambda^k\right] V^{-1} = V e^{\Lambda} V^{-1}\]
    where we can compute $e^{\Lambda}$ easily: 
    \[e^{\Lambda} = \begin{pmatrix} e^{\lambda_1} &  &  \\  & \ddots &  \\  &  & e^{\lambda_n} \end{pmatrix} \]

    \subparag{Summary}{
        To sum up, we have: 
        \[\bvec{x}'\left(t\right) = A \bvec{x}\left(t\right) \iff \bvec{x}\left(t\right) = e^{A t} \bvec{c} = V e^{\Lambda} V^{-1} \bvec{c}\]
        where $At = V e^{\Lambda} V^{-1}$ is its eigendecomposition.
    }
    
    \subparag{Personal remark}{
        There is a great 3Blue1Brown video on this subject:
        \begin{center}
            \url{https://youtu.be/O85OWBJ2ayo}
        \end{center}
    }
}


\parag{Remarks}{
    We can notice that in fact eigenvalue-eigenvector pairs are actually a triple $\left(\bvec{x}, \bvec{y}, \lambda\right)$ where we have a left eigenvector and a righteignevector, respectively: 
    \[\bvec{y}^T A = \lambda \bvec{y}^T, \mathspace A \bvec{x} = \lambda \bvec{x}\]
    
    Also, in fact complex arithmetic is generally required, meaning that we need to work with complex arithmetic. Another problem is that methods can be very unstable if eigenvalues are not well separated. Finally, none of this makes sense for non-square matrices.

    We show that this decomposition yields to a lot of difficulties, which leads us to the following decomposition.
}

\subsection{Singular Value Decomposition}
\parag{Remark}{
    In general, if we are stuck with a linear algebra problem, we can replace a matrix by its SVD, and it will often helps us.
}

\parag{Goal}{
    The goal is to have two orthogonal matrices $U, V$ and a diagonal matrix $\Sigma$ such that: 
    \[A = U\Sigma V^T\]
    
    Note that this costs around 5 to 10 times more than LU factorisation, which means that we should not compute it if we don't need it.

    \subparag{Remarks}{
        This decomposition is really great: it works for any matrix (non-square, non-symmetric, singular, and so on), the computations are stable (we don't need pivoting), it involves only orthogonal and diagonal matrices (which are very easy to invert), it has impeccable numerical properties and it does not require any complex arithmetic.
    }
    
    \subparag{Ellipses}{
        For a $2\times2$ matrix, for which the vectors $\bvec{v}_1, \bvec{v}_2$ are the columns of the matrix $V$ and the vectors $\bvec{u}_1, \bvec{u}_2$, we can draw the following ellipses:
        \imagehere[0.5]{SVDEllipse.png}

        Thus, when applying the decomposition, we transform our vector onto the blue ellipse, apply it a non-uniform magnification, and finally put it onto the red ellipse.
    }
}

\parag{Non-square systems}{
    When working with non-square matrices, we get the following forms:
    \imagehere[0.5]{SVDMatricesShape.png}

    As for the QR decomposition, we can get an economic SVD, by removing all zeroes and all elements that would have been multiplied by 0 anyway.
}

\parag{Matrix norm}{
    We can compute the matrix norm very easily from an SVD: 
    \[\left\|A\right\|_2 = \sigma_1\]

    \subparag{Remark}{
        The singular values are ordered by convention, meaning that $\sigma_1$ is the biggest one.
    }
    
}

\parag{Condition value}{
    Also, we can compute the conditioning value easily from an SVD: 
    \[\cond\left(A\right) = \left\|A\right\| \cdot  \left\|A^{-1}\right\| = \frac{\sigma_1}{\sigma_n}\]
    
    \subparag{Remark}{
        Note that this is not really the best way to compute the condition value since the SVD is expensive, but it works (yeah you start to get the idea, this course is not really understanding algorithms but more concepts).
    }
}

\parag{Outer product}{
    We can find the following way to express the SVD: 
    \[A = U \Sigma V^T = \sum_{i=1}^{n} \sigma \bvec{u}_i \bvec{v}_i^T\]
    
    Where $\bvec{u} \bvec{v}^T$ is an outer product: 
    \[\bvec{u} \bvec{v}^T = \begin{pmatrix} u_1 v_1 & u_1 v_2 & \cdots & u_1 v_n \\ u_2 v_1 & u_2 v_2 & \cdots & u_2 v_n \\ \vdots & \vdots & \ddots & \vdots \\ u_m v_1 & u_m v_2 & \cdots & u_m v_n \end{pmatrix} \]
    
    It has only a rank equal to 1, since all its columns are linearly dependent.
}

\parag{Inverse}{
    If $A$ is a square matrix, knowing that $A = U \Sigma V^T$, we can compute its inverse: 
    \[A^{-1} = V \Sigma^{-1} U^T = \sum_{i=1}^{n} \frac{1}{\sigma_i} \bvec{v}_i \bvec{u}_i^T\]

    Also, if $A$ is not a square matrix, we can compute its Moore-Penrose pseudoinverse by a very similar computation:
    \[A^+ = \sum_{i=1}^{\min\left\{n, m\right\}} \bvec{v}_i \bvec{u}_i^T k_i\]
    where $k_i = \frac{1}{\sigma_i}$ if $\sigma_i \neq 0$, and 0 otherwise.
    
    When we have a full-rank matrix, this is identical to the inverse. For over constrained problems (tall matrices), the pseudoinverse gives the least squares solution. For underconstrainded problems (wide matrices), it gives a minimum-norm solution.
}

\parag{Eckart-Young theorem}{
    If we compute the SVD of a matrix $A$, but truncating the largest $k$ singular values using the SVD, then this is a pretty good approximation.

    Indeed, the matrix $\hat{A}$ got with this approximation minimises both $\left\|A - \hat{A}\right\|_2$ and $\left\|A - \hat{A}\right\|_F$, where the second norm is the Frobenius norm (computed by stacking all the values of our matrix into a column vector, and computing its 2-norm).

    \subparag{Low-rank approximation}{
        In other words, by computing the following matrix, we get a \important{low-rank approximation}, which provides the best possible approximation using outer products: 
        \[\hat{A} = \sum_{i=1}^{k_{max}} \sigma_i \bvec{u}_i \bvec{v}_i^T\]
    }
}

\parag{Regularisation strategy}{
    The distribution of singular values typically looks like:
    \imagehere[0.65]{DistributionSingularValues.png}

    However, when we are computing the inverse, we compute the reciprocal of the singular values, which might explode for very small values. However, we can do regularisation by computing the inverse of a low-rank approximation of our matrix: 
    \[\hat{A}^{-1} = \sum_{i=1}^{k} \frac{1}{\sigma_i} \bvec{v}_i \bvec{u}_i^T\]
    
    \imagehere[0.65]{DistributionSingularValuesStopped.png}

    This conditioning works much better than Tikhonov regularisation.

    \subparag{Remark}{
        We will be able to experiment with this idea in the homework on the shadows.
    }
}

\end{document}

